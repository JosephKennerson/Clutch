<<<<<<< HEAD
[2015-10-15 00:00:11,460][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:00:11,461][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:00:41,459][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:01:11,464][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:01:11,464][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:01:41,466][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:02:11,469][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:02:11,469][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:02:41,472][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:03:11,475][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:03:11,475][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:03:41,479][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:04:11,479][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:04:11,479][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:04:41,484][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:05:11,487][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:05:11,488][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:05:41,489][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:06:11,490][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:06:11,490][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:06:41,490][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:07:11,489][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:07:11,489][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:07:41,490][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:08:11,491][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:08:11,491][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:08:41,496][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:09:11,497][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:09:11,497][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:09:41,500][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:10:11,501][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:10:11,501][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:10:37,895][INFO ][node                     ] [Diamond Lil] stopping ...
[2015-10-15 00:10:37,912][INFO ][node                     ] [Diamond Lil] stopped
[2015-10-15 00:10:37,912][INFO ][node                     ] [Diamond Lil] closing ...
[2015-10-15 00:10:37,916][INFO ][node                     ] [Diamond Lil] closed
[2015-10-15 00:10:52,225][INFO ][node                     ] [Scimitar] version[1.7.1], pid[64222], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-15 00:10:52,226][INFO ][node                     ] [Scimitar] initializing ...
[2015-10-15 00:10:52,293][INFO ][plugins                  ] [Scimitar] loaded [], sites []
[2015-10-15 00:10:52,327][INFO ][env                      ] [Scimitar] using [1] data paths, mounts [[/ (/dev/disk0s2)]], net usable_space [25.3gb], net total_space [352.9gb], types [hfs]
[2015-10-15 00:10:54,277][INFO ][node                     ] [Scimitar] initialized
[2015-10-15 00:10:54,277][INFO ][node                     ] [Scimitar] starting ...
[2015-10-15 00:10:54,340][INFO ][transport                ] [Scimitar] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.8:9300]}
[2015-10-15 00:10:54,358][INFO ][discovery                ] [Scimitar] elasticsearch/e3fw3_EjRvum9iWV9fTcAw
[2015-10-15 00:10:58,136][INFO ][cluster.service          ] [Scimitar] new_master [Scimitar][e3fw3_EjRvum9iWV9fTcAw][Beepo-Retina.local][inet[/192.168.1.8:9300]], reason: zen-disco-join (elected_as_master)
[2015-10-15 00:10:58,167][INFO ][http                     ] [Scimitar] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.8:9200]}
[2015-10-15 00:10:58,168][INFO ][node                     ] [Scimitar] started
[2015-10-15 00:10:58,193][INFO ][gateway                  ] [Scimitar] recovered [2] indices into cluster_state
[2015-10-15 00:11:28,147][WARN ][cluster.routing.allocation.decider] [Scimitar] high disk watermark [90%] exceeded on [e3fw3_EjRvum9iWV9fTcAw][Scimitar] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:11:28,147][INFO ][cluster.routing.allocation.decider] [Scimitar] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:11:58,144][WARN ][cluster.routing.allocation.decider] [Scimitar] high disk watermark [90%] exceeded on [e3fw3_EjRvum9iWV9fTcAw][Scimitar] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:12:28,147][WARN ][cluster.routing.allocation.decider] [Scimitar] high disk watermark [90%] exceeded on [e3fw3_EjRvum9iWV9fTcAw][Scimitar] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:12:28,147][INFO ][cluster.routing.allocation.decider] [Scimitar] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:12:50,771][DEBUG][action.index             ] [Scimitar] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-10-15 00:12:58,151][WARN ][cluster.routing.allocation.decider] [Scimitar] high disk watermark [90%] exceeded on [e3fw3_EjRvum9iWV9fTcAw][Scimitar] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:13:28,155][WARN ][cluster.routing.allocation.decider] [Scimitar] high disk watermark [90%] exceeded on [e3fw3_EjRvum9iWV9fTcAw][Scimitar] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:13:28,156][INFO ][cluster.routing.allocation.decider] [Scimitar] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:13:36,272][INFO ][node                     ] [Scimitar] stopping ...
[2015-10-15 00:13:36,284][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
	at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)
	at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
	at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
	at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
	at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)
	at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
	at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.finishAsFailed(TransportShardReplicationOperationAction.java:536)
	at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$3.onClusterServiceClose(TransportShardReplicationOperationAction.java:509)
	at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onClose(ClusterStateObserver.java:217)
	at org.elasticsearch.cluster.service.InternalClusterService.doStop(InternalClusterService.java:174)
	at org.elasticsearch.common.component.AbstractLifecycleComponent.stop(AbstractLifecycleComponent.java:105)
	at org.elasticsearch.node.internal.InternalNode.stop(InternalNode.java:310)
	at org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:334)
	at org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:82)
[2015-10-15 00:13:36,294][INFO ][node                     ] [Scimitar] stopped
[2015-10-15 00:13:36,294][INFO ][node                     ] [Scimitar] closing ...
[2015-10-15 00:13:36,298][INFO ][node                     ] [Scimitar] closed
[2015-10-15 00:14:30,277][INFO ][node                     ] [Justin Hammer] version[1.7.1], pid[64862], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-15 00:14:30,278][INFO ][node                     ] [Justin Hammer] initializing ...
[2015-10-15 00:14:30,343][INFO ][plugins                  ] [Justin Hammer] loaded [], sites []
[2015-10-15 00:14:30,377][INFO ][env                      ] [Justin Hammer] using [1] data paths, mounts [[/ (/dev/disk0s2)]], net usable_space [25.3gb], net total_space [352.9gb], types [hfs]
[2015-10-15 00:14:32,326][INFO ][node                     ] [Justin Hammer] initialized
[2015-10-15 00:14:32,327][INFO ][node                     ] [Justin Hammer] starting ...
[2015-10-15 00:14:32,388][INFO ][transport                ] [Justin Hammer] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.8:9300]}
[2015-10-15 00:14:32,404][INFO ][discovery                ] [Justin Hammer] elasticsearch/3StrIZ3CQbm8qH4vW5aBLQ
[2015-10-15 00:14:36,183][INFO ][cluster.service          ] [Justin Hammer] new_master [Justin Hammer][3StrIZ3CQbm8qH4vW5aBLQ][Beepo-Retina.local][inet[/192.168.1.8:9300]], reason: zen-disco-join (elected_as_master)
[2015-10-15 00:14:36,216][INFO ][http                     ] [Justin Hammer] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.8:9200]}
[2015-10-15 00:14:36,217][INFO ][node                     ] [Justin Hammer] started
[2015-10-15 00:14:36,238][INFO ][gateway                  ] [Justin Hammer] recovered [2] indices into cluster_state
[2015-10-15 00:15:06,196][WARN ][cluster.routing.allocation.decider] [Justin Hammer] high disk watermark [90%] exceeded on [3StrIZ3CQbm8qH4vW5aBLQ][Justin Hammer] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:15:06,196][INFO ][cluster.routing.allocation.decider] [Justin Hammer] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:15:36,191][WARN ][cluster.routing.allocation.decider] [Justin Hammer] high disk watermark [90%] exceeded on [3StrIZ3CQbm8qH4vW5aBLQ][Justin Hammer] free: 25.2gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:15:38,854][DEBUG][action.index             ] [Justin Hammer] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-10-15 00:16:06,196][WARN ][cluster.routing.allocation.decider] [Justin Hammer] high disk watermark [90%] exceeded on [3StrIZ3CQbm8qH4vW5aBLQ][Justin Hammer] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:16:06,196][INFO ][cluster.routing.allocation.decider] [Justin Hammer] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:16:26,266][INFO ][node                     ] [Justin Hammer] stopping ...
[2015-10-15 00:16:26,278][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
	at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)
	at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
	at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
	at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
	at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)
	at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
	at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.finishAsFailed(TransportShardReplicationOperationAction.java:536)
	at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$3.onClusterServiceClose(TransportShardReplicationOperationAction.java:509)
	at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onClose(ClusterStateObserver.java:217)
	at org.elasticsearch.cluster.service.InternalClusterService.doStop(InternalClusterService.java:174)
	at org.elasticsearch.common.component.AbstractLifecycleComponent.stop(AbstractLifecycleComponent.java:105)
	at org.elasticsearch.node.internal.InternalNode.stop(InternalNode.java:310)
	at org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:334)
	at org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:82)
[2015-10-15 00:16:26,287][INFO ][node                     ] [Justin Hammer] stopped
[2015-10-15 00:16:26,288][INFO ][node                     ] [Justin Hammer] closing ...
[2015-10-15 00:16:26,292][INFO ][node                     ] [Justin Hammer] closed
[2015-10-15 00:52:12,116][INFO ][node                     ] [Bedlam] version[1.7.1], pid[1538], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-15 00:52:12,116][INFO ][node                     ] [Bedlam] initializing ...
[2015-10-15 00:52:12,235][INFO ][plugins                  ] [Bedlam] loaded [], sites []
[2015-10-15 00:52:12,293][INFO ][env                      ] [Bedlam] using [1] data paths, mounts [[/ (/dev/disk0s2)]], net usable_space [25.4gb], net total_space [352.9gb], types [hfs]
[2015-10-15 00:52:16,792][INFO ][node                     ] [Bedlam] initialized
[2015-10-15 00:52:16,793][INFO ][node                     ] [Bedlam] starting ...
[2015-10-15 00:52:17,239][INFO ][transport                ] [Bedlam] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.8:9300]}
[2015-10-15 00:52:17,277][INFO ][discovery                ] [Bedlam] elasticsearch/V_X2kzZmRbejXbcvdoPnTg
[2015-10-15 00:52:21,062][INFO ][cluster.service          ] [Bedlam] new_master [Bedlam][V_X2kzZmRbejXbcvdoPnTg][Beepo-Retina.local][inet[/192.168.1.8:9300]], reason: zen-disco-join (elected_as_master)
[2015-10-15 00:52:21,084][INFO ][http                     ] [Bedlam] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.8:9200]}
[2015-10-15 00:52:21,085][INFO ][node                     ] [Bedlam] started
[2015-10-15 00:52:21,122][INFO ][gateway                  ] [Bedlam] recovered [2] indices into cluster_state
[2015-10-15 00:52:51,074][WARN ][cluster.routing.allocation.decider] [Bedlam] high disk watermark [90%] exceeded on [V_X2kzZmRbejXbcvdoPnTg][Bedlam] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:52:51,074][INFO ][cluster.routing.allocation.decider] [Bedlam] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:53:21,072][WARN ][cluster.routing.allocation.decider] [Bedlam] high disk watermark [90%] exceeded on [V_X2kzZmRbejXbcvdoPnTg][Bedlam] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:53:51,074][WARN ][cluster.routing.allocation.decider] [Bedlam] high disk watermark [90%] exceeded on [V_X2kzZmRbejXbcvdoPnTg][Bedlam] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:53:51,074][INFO ][cluster.routing.allocation.decider] [Bedlam] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:54:21,079][WARN ][cluster.routing.allocation.decider] [Bedlam] high disk watermark [90%] exceeded on [V_X2kzZmRbejXbcvdoPnTg][Bedlam] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:54:40,336][INFO ][node                     ] [Bedlam] stopping ...
[2015-10-15 00:54:40,360][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
	at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)
	at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
	at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
	at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
	at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)
	at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
	at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.finishAsFailed(TransportShardReplicationOperationAction.java:536)
	at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$3.onClusterServiceClose(TransportShardReplicationOperationAction.java:509)
	at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onClose(ClusterStateObserver.java:217)
	at org.elasticsearch.cluster.service.InternalClusterService.doStop(InternalClusterService.java:174)
	at org.elasticsearch.common.component.AbstractLifecycleComponent.stop(AbstractLifecycleComponent.java:105)
	at org.elasticsearch.node.internal.InternalNode.stop(InternalNode.java:310)
	at org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:334)
	at org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:82)
[2015-10-15 00:54:40,374][INFO ][node                     ] [Bedlam] stopped
[2015-10-15 00:54:40,374][INFO ][node                     ] [Bedlam] closing ...
[2015-10-15 00:54:40,380][INFO ][node                     ] [Bedlam] closed
=======

[2015-10-14 14:16:27,431][INFO ][cluster.service          ] [Raza] added {[David Cannon][w2wuzrOJSJGQ0FKwvYYF4Q][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],}, reason: zen-disco-receive(join from node[[David Cannon][w2wuzrOJSJGQ0FKwvYYF4Q][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-14 14:16:27,864][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 746mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:16:27,871][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:16:36,123][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 745.4mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:17:06,094][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 729.1mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:17:36,330][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 728.2mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:17:36,340][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:18:06,132][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 727.8mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:18:36,109][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 726.7mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:19:06,259][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 726.6mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:19:06,264][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:19:36,144][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 726.6mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:20:06,119][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 726.6mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:20:36,130][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 726.2mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:20:36,141][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:21:06,153][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 725.9mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:21:36,254][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 725.7mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:21:36,256][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:22:06,135][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 724.6mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:22:36,157][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 724.5mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:23:06,268][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 724.5mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:23:06,271][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:23:36,680][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 639.4mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:24:06,170][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 639.4mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:24:36,281][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 639.1mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:24:36,285][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:25:06,653][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 639.1mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:25:36,181][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 643.1mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:26:06,287][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 643.2mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:26:06,290][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:26:36,220][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 643.2mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:27:06,176][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 643.2mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:27:36,285][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 643.2mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:27:36,289][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:28:06,175][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 642.9mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:28:36,201][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 642.7mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:29:06,222][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 636.9mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:29:06,225][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:29:36,496][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 636.9mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:30:06,213][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 636.9mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:30:36,242][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 636.8mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:30:36,245][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:31:06,519][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 636.8mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:31:36,222][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 636.8mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:32:06,221][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 636.5mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:32:06,225][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:32:36,364][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 636.6mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:33:06,684][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 636.6mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:33:06,687][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:33:36,340][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 636.5mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:34:06,331][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 640.9mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:34:36,550][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 575.6mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:34:36,553][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:35:06,253][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 570.2mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:35:36,270][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 565.5mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:36:06,368][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 566.2mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:36:06,371][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:36:36,354][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 571.5mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:37:06,466][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 568.6mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:37:06,477][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:37:36,876][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 581.1mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:38:06,316][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 576mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:38:36,278][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 575.5mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:38:36,287][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:39:06,283][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 571.8mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:39:36,290][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 571.7mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:39:36,294][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:40:06,301][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 574.4mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:40:36,310][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 552.1mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:40:36,312][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:41:06,517][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 574.9mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:41:36,321][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 553.2mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:41:36,336][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:42:06,315][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 555.9mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:42:36,369][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 555.2mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:42:36,379][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:43:06,364][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 555.2mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:43:36,348][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 555.2mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:44:06,340][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 632.2mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:44:06,341][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:44:36,348][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 632.1mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:45:06,450][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 632.1mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:45:06,461][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:45:36,364][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 632mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:46:06,353][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 600mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:46:36,364][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 619.4mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:46:36,375][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:47:06,873][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 633.8mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:47:36,427][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 612.7mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:47:36,431][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:48:06,472][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 720.3mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:48:36,378][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 695.9mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:49:06,443][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 708.8mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:49:06,444][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:49:36,482][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 700.4mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:50:06,481][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 699.9mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:50:06,489][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:50:36,402][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 703.6mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:51:06,500][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 692.4mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:51:06,511][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:51:36,596][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 714.2mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:52:06,420][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 714.2mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:52:36,502][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 718mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:52:36,503][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:53:06,615][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 717.8mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:53:36,801][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 693.7mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:53:36,804][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:54:06,428][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 693.6mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:54:36,571][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 697.3mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:55:07,000][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 691mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:55:07,003][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:55:36,449][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 686.4mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:56:06,626][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 691.5mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:56:36,495][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 689.4mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:56:36,498][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:57:06,452][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 707.9mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:57:36,668][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 700.4mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:57:36,676][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:58:06,476][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 705.2mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:58:36,656][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 692.5mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:59:06,540][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 691.8mb[0.2%], shards will be relocated away from this node
[2015-10-14 14:59:06,542][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:59:36,481][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 677.3mb[0.2%], shards will be relocated away from this node
[2015-10-14 15:00:06,605][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 576.2mb[0.2%], shards will be relocated away from this node
[2015-10-14 15:00:06,608][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 15:00:36,680][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 958.5mb[0.4%], shards will be relocated away from this node
[2015-10-14 15:01:06,779][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 954.2mb[0.4%], shards will be relocated away from this node
[2015-10-14 15:01:06,790][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 15:01:36,505][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 954.3mb[0.4%], shards will be relocated away from this node
[2015-10-14 15:02:06,650][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 957.5mb[0.4%], shards will be relocated away from this node
[2015-10-14 15:02:36,798][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 959.3mb[0.4%], shards will be relocated away from this node
[2015-10-14 15:02:36,802][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 15:03:06,582][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 941.3mb[0.3%], shards will be relocated away from this node
[2015-10-14 15:03:36,697][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 948.3mb[0.3%], shards will be relocated away from this node
[2015-10-14 15:04:06,521][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 948.2mb[0.3%], shards will be relocated away from this node
[2015-10-14 15:04:06,523][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 15:04:36,540][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 963.6mb[0.4%], shards will be relocated away from this node
[2015-10-14 15:05:06,535][WARN ][cluster.routing.allocation.decider] [Raza] high disk watermark [90%] exceeded on [w2wuzrOJSJGQ0FKwvYYF4Q][David Cannon] free: 953.9mb[0.4%], shards will be relocated away from this node
[2015-10-14 15:05:06,536][INFO ][cluster.routing.allocation.decider] [Raza] high disk watermark exceeded on one or more nodes, rerouting shards

[2015-10-13 18:49:53,877][INFO ][node                     ] [Gee] version[1.7.1], pid[89101], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-13 18:49:53,877][INFO ][node                     ] [Gee] initializing ...
[2015-10-13 18:49:54,052][INFO ][plugins                  ] [Gee] loaded [], sites []
[2015-10-13 18:49:54,135][INFO ][env                      ] [Gee] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [133.4gb], net total_space [232.6gb], types [hfs]
[2015-10-13 18:49:58,374][INFO ][node                     ] [Gee] initialized
[2015-10-13 18:49:58,375][INFO ][node                     ] [Gee] starting ...
[2015-10-13 18:49:58,516][INFO ][transport                ] [Gee] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.145:9300]}
[2015-10-13 18:49:58,550][INFO ][discovery                ] [Gee] elasticsearch/pbarfueRT1yKMg-3ogP1eQ
[2015-10-13 18:50:02,343][INFO ][cluster.service          ] [Gee] new_master [Gee][pbarfueRT1yKMg-3ogP1eQ][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]], reason: zen-disco-join (elected_as_master)
[2015-10-13 18:50:02,366][INFO ][http                     ] [Gee] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.145:9200]}
[2015-10-13 18:50:02,367][INFO ][node                     ] [Gee] started
[2015-10-13 18:50:02,391][INFO ][gateway                  ] [Gee] recovered [0] indices into cluster_state
[2015-10-13 18:51:04,857][INFO ][node                     ] [Gee] stopping ...
[2015-10-13 18:51:04,927][INFO ][node                     ] [Gee] stopped
[2015-10-13 18:51:04,928][INFO ][node                     ] [Gee] closing ...
[2015-10-13 18:51:04,940][INFO ][node                     ] [Gee] closed
[2015-10-13 19:00:36,706][INFO ][node                     ] [Arc] version[1.7.1], pid[89397], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-13 19:00:36,709][INFO ][node                     ] [Arc] initializing ...
[2015-10-13 19:00:36,870][INFO ][plugins                  ] [Arc] loaded [], sites []
[2015-10-13 19:00:36,956][INFO ][env                      ] [Arc] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [133.4gb], net total_space [232.6gb], types [hfs]
[2015-10-13 19:00:41,029][INFO ][node                     ] [Arc] initialized
[2015-10-13 19:00:41,029][INFO ][node                     ] [Arc] starting ...
[2015-10-13 19:00:41,200][INFO ][transport                ] [Arc] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.145:9300]}
[2015-10-13 19:00:41,239][INFO ][discovery                ] [Arc] elasticsearch/HN8u2lrnSrycq6pQAqY3AA
[2015-10-13 19:00:45,035][INFO ][cluster.service          ] [Arc] new_master [Arc][HN8u2lrnSrycq6pQAqY3AA][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]], reason: zen-disco-join (elected_as_master)
[2015-10-13 19:00:45,058][INFO ][http                     ] [Arc] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.145:9200]}
[2015-10-13 19:00:45,058][INFO ][node                     ] [Arc] started
[2015-10-13 19:00:45,069][INFO ][gateway                  ] [Arc] recovered [0] indices into cluster_state
[2015-10-13 19:02:15,825][INFO ][node                     ] [Arc] stopping ...
[2015-10-13 19:02:16,399][INFO ][node                     ] [Arc] stopped
[2015-10-13 19:02:16,402][INFO ][node                     ] [Arc] closing ...
[2015-10-13 19:02:16,758][INFO ][node                     ] [Arc] closed
[2015-10-13 19:17:20,857][INFO ][node                     ] [Raza] version[1.7.1], pid[90335], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-13 19:17:20,859][INFO ][node                     ] [Raza] initializing ...
[2015-10-13 19:17:21,015][INFO ][plugins                  ] [Raza] loaded [], sites []
[2015-10-13 19:17:21,089][INFO ][env                      ] [Raza] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [133.4gb], net total_space [232.6gb], types [hfs]
[2015-10-13 19:17:24,709][INFO ][node                     ] [Raza] initialized
[2015-10-13 19:17:24,709][INFO ][node                     ] [Raza] starting ...
[2015-10-13 19:17:24,907][INFO ][transport                ] [Raza] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.145:9300]}
[2015-10-13 19:17:24,952][INFO ][discovery                ] [Raza] elasticsearch/qCFFzUjQS12EsdxOzv8gig
[2015-10-13 19:17:28,833][INFO ][cluster.service          ] [Raza] new_master [Raza][qCFFzUjQS12EsdxOzv8gig][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]], reason: zen-disco-join (elected_as_master)
[2015-10-13 19:17:28,863][INFO ][http                     ] [Raza] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.145:9200]}
[2015-10-13 19:17:28,863][INFO ][node                     ] [Raza] started
[2015-10-13 19:17:28,871][INFO ][gateway                  ] [Raza] recovered [0] indices into cluster_state
[2015-10-13 19:18:03,141][INFO ][cluster.metadata         ] [Raza] [events] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [event]
[2015-10-13 19:18:03,750][INFO ][cluster.metadata         ] [Raza] [events] update_mapping [event] (dynamic)
[2015-10-14 13:56:45,731][INFO ][node                     ] [Alicia Masters] version[1.7.1], pid[22381], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-14 13:56:45,732][INFO ][node                     ] [Alicia Masters] initializing ...
[2015-10-14 13:56:45,909][INFO ][plugins                  ] [Alicia Masters] loaded [], sites []
[2015-10-14 13:56:45,997][INFO ][env                      ] [Alicia Masters] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [784mb], net total_space [232.6gb], types [hfs]
[2015-10-14 13:56:49,768][INFO ][node                     ] [Alicia Masters] initialized
[2015-10-14 13:56:49,768][INFO ][node                     ] [Alicia Masters] starting ...
[2015-10-14 13:56:49,993][INFO ][transport                ] [Alicia Masters] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.126:9300]}
[2015-10-14 13:56:50,058][INFO ][discovery                ] [Alicia Masters] elasticsearch/YN-QYXl5RWu7YGNYoxPE4Q
[2015-10-14 13:56:53,874][INFO ][cluster.service          ] [Alicia Masters] new_master [Alicia Masters][YN-QYXl5RWu7YGNYoxPE4Q][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]], reason: zen-disco-join (elected_as_master)
[2015-10-14 13:56:53,897][INFO ][http                     ] [Alicia Masters] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.126:9200]}
[2015-10-14 13:56:53,897][INFO ][node                     ] [Alicia Masters] started
[2015-10-14 13:56:53,935][INFO ][gateway                  ] [Alicia Masters] recovered [1] indices into cluster_state
[2015-10-14 13:57:23,908][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 784.2mb[0.3%], shards will be relocated away from this node
[2015-10-14 13:57:23,909][INFO ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 13:57:53,890][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 784.1mb[0.3%], shards will be relocated away from this node
[2015-10-14 13:58:23,892][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 783.7mb[0.3%], shards will be relocated away from this node
[2015-10-14 13:58:53,898][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 772.9mb[0.3%], shards will be relocated away from this node
[2015-10-14 13:58:53,899][INFO ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 13:59:23,905][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 740.8mb[0.3%], shards will be relocated away from this node
[2015-10-14 13:59:53,907][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 740.3mb[0.3%], shards will be relocated away from this node
[2015-10-14 13:59:53,907][INFO ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:00:23,910][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 739.7mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:00:53,913][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 739mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:00:53,913][INFO ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:01:23,917][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 739mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:01:53,919][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 738.9mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:01:53,927][INFO ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:02:23,925][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 738.9mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:02:53,928][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 738.9mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:03:23,932][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 738.5mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:03:23,941][INFO ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:03:53,935][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 737.8mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:04:23,940][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 737.8mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:04:53,940][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 737.8mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:04:53,940][INFO ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:05:23,942][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 737.8mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:05:53,945][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 737.8mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:05:53,953][INFO ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-14 14:06:23,951][WARN ][cluster.routing.allocation.decider] [Alicia Masters] high disk watermark [90%] exceeded on [YN-QYXl5RWu7YGNYoxPE4Q][Alicia Masters] free: 737.7mb[0.3%], shards will be relocated away from this node
[2015-10-14 14:16:19,426][INFO ][node                     ] [David Cannon] version[1.7.1], pid[23559], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-14 14:16:19,427][INFO ][node                     ] [David Cannon] initializing ...
[2015-10-14 14:16:19,599][INFO ][plugins                  ] [David Cannon] loaded [], sites []
[2015-10-14 14:16:19,659][INFO ][env                      ] [David Cannon] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [746mb], net total_space [232.6gb], types [hfs]
[2015-10-14 14:16:23,350][INFO ][node                     ] [David Cannon] initialized
[2015-10-14 14:16:23,351][INFO ][node                     ] [David Cannon] starting ...
[2015-10-14 14:16:23,537][INFO ][transport                ] [David Cannon] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.126:9300]}
[2015-10-14 14:16:23,585][INFO ][discovery                ] [David Cannon] elasticsearch/w2wuzrOJSJGQ0FKwvYYF4Q
[2015-10-14 14:16:27,630][INFO ][cluster.service          ] [David Cannon] detected_master [Raza][qCFFzUjQS12EsdxOzv8gig][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]], added {[Raza][qCFFzUjQS12EsdxOzv8gig][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]],}, reason: zen-disco-receive(from master [[Raza][qCFFzUjQS12EsdxOzv8gig][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]]])
[2015-10-14 14:16:27,660][INFO ][http                     ] [David Cannon] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.126:9200]}
<<<<<<< HEAD
[2015-10-14 14:16:27,660][INFO ][node                     ] [David Cannon] started
[2015-10-14 21:37:12,621][INFO ][node                     ] [Kraven the Hunter] version[1.7.1], pid[28614], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-14 21:37:12,623][INFO ][node                     ] [Kraven the Hunter] initializing ...
[2015-10-14 21:37:12,700][INFO ][plugins                  ] [Kraven the Hunter] loaded [], sites []
[2015-10-14 21:37:12,741][INFO ][env                      ] [Kraven the Hunter] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [149gb], net total_space [232.6gb], types [hfs]
[2015-10-14 21:37:14,991][INFO ][node                     ] [Kraven the Hunter] initialized
[2015-10-14 21:37:14,992][INFO ][node                     ] [Kraven the Hunter] starting ...
[2015-10-14 21:37:15,081][INFO ][transport                ] [Kraven the Hunter] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.164:9300]}
[2015-10-14 21:37:15,105][INFO ][discovery                ] [Kraven the Hunter] elasticsearch/SdqJ8RO3Te6hJ5PSXRkRKw
[2015-10-14 21:37:18,558][INFO ][cluster.service          ] [Kraven the Hunter] detected_master [Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]], added {[Danger][ruyQPlLlTum_W-_UpF7a2A][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]],[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]],}, reason: zen-disco-receive(from master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-14 21:37:18,603][INFO ][http                     ] [Kraven the Hunter] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.164:9200]}
[2015-10-14 21:37:18,603][INFO ][node                     ] [Kraven the Hunter] started
[2015-10-14 22:16:25,067][INFO ][discovery.zen            ] [Kraven the Hunter] master_left [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]], reason [do not exists on master, act as master failure]
[2015-10-14 22:16:25,091][WARN ][discovery.zen            ] [Kraven the Hunter] master left (reason = do not exists on master, act as master failure), current nodes: {[Kraven the Hunter][SdqJ8RO3Te6hJ5PSXRkRKw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],[Danger][ruyQPlLlTum_W-_UpF7a2A][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]],}
[2015-10-14 22:16:25,091][INFO ][cluster.service          ] [Kraven the Hunter] removed {[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]],}, reason: zen-disco-master_failed ([Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]])
[2015-10-14 22:16:28,382][WARN ][discovery.zen.ping.multicast] [Kraven the Hunter] received ping response ping_response{node [[Danger][ruyQPlLlTum_W-_UpF7a2A][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]]], id[55], master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]], hasJoinedOnce [true], cluster_name[elasticsearch]} with no matching id [2]
[2015-10-14 22:16:28,383][WARN ][discovery.zen.ping.multicast] [Kraven the Hunter] received ping response ping_response{node [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]], id[19], master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]], hasJoinedOnce [true], cluster_name[elasticsearch]} with no matching id [2]
[2015-10-14 22:16:30,116][INFO ][cluster.service          ] [Kraven the Hunter] detected_master [Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]], added {[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]],}, reason: zen-disco-receive(from master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-14 23:07:50,256][INFO ][discovery.zen            ] [Kraven the Hunter] master_left [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]], reason [do not exists on master, act as master failure]
[2015-10-14 23:07:50,259][WARN ][discovery.zen            ] [Kraven the Hunter] master left (reason = do not exists on master, act as master failure), current nodes: {[Kraven the Hunter][SdqJ8RO3Te6hJ5PSXRkRKw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],[Danger][ruyQPlLlTum_W-_UpF7a2A][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]],}
[2015-10-14 23:07:50,260][INFO ][cluster.service          ] [Kraven the Hunter] removed {[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]],}, reason: zen-disco-master_failed ([Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]])
[2015-10-14 23:07:54,611][WARN ][discovery.zen.ping.multicast] [Kraven the Hunter] received ping response ping_response{node [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]], id[22], master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]], hasJoinedOnce [true], cluster_name[elasticsearch]} with no matching id [3]
[2015-10-14 23:07:54,611][WARN ][discovery.zen.ping.multicast] [Kraven the Hunter] received ping response ping_response{node [[Danger][ruyQPlLlTum_W-_UpF7a2A][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]]], id[61], master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]], hasJoinedOnce [true], cluster_name[elasticsearch]} with no matching id [3]
[2015-10-14 23:07:55,628][INFO ][cluster.service          ] [Kraven the Hunter] detected_master [Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]], added {[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]],}, reason: zen-disco-receive(from master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-14 23:13:49,878][INFO ][node                     ] [Gaza] version[1.7.1], pid[30003], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-14 23:13:49,878][INFO ][node                     ] [Gaza] initializing ...
[2015-10-14 23:13:49,964][INFO ][plugins                  ] [Gaza] loaded [], sites []
[2015-10-14 23:13:50,012][INFO ][env                      ] [Gaza] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [148.8gb], net total_space [232.6gb], types [hfs]
[2015-10-14 23:13:52,508][INFO ][node                     ] [Gaza] initialized
[2015-10-14 23:13:52,508][INFO ][node                     ] [Gaza] starting ...
[2015-10-14 23:13:52,609][INFO ][transport                ] [Gaza] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.164:9300]}
[2015-10-14 23:13:52,637][INFO ][discovery                ] [Gaza] elasticsearch/3OXKOIagQ72TtKlDtWwhhQ
[2015-10-14 23:13:55,849][INFO ][cluster.service          ] [Gaza] detected_master [Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]], added {[Danger][ruyQPlLlTum_W-_UpF7a2A][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]],[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]],}, reason: zen-disco-receive(from master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-14 23:13:55,889][INFO ][http                     ] [Gaza] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.164:9200]}
[2015-10-14 23:13:55,889][INFO ][node                     ] [Gaza] started
=======
[2015-10-14 14:16:27,660][INFO ][node                     ] [David Cannon] started[2015-10-14 16:21:16,525][INFO ][node                     ] [Shriker] version[1.7.1], pid[6105], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-14 16:21:16,526][INFO ][node                     ] [Shriker] initializing ...
[2015-10-14 16:21:16,683][INFO ][plugins                  ] [Shriker] loaded [], sites []
[2015-10-14 16:21:16,775][INFO ][env                      ] [Shriker] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [133.2gb], net total_space [232.6gb], types [hfs]
[2015-10-14 16:21:21,247][INFO ][node                     ] [Shriker] initialized
[2015-10-14 16:21:21,248][INFO ][node                     ] [Shriker] starting ...
[2015-10-14 16:21:21,611][INFO ][transport                ] [Shriker] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.145:9300]}
[2015-10-14 16:21:21,666][INFO ][discovery                ] [Shriker] elasticsearch/vU_nG2vfSC-sFsZb60bDGQ
[2015-10-14 16:21:25,345][INFO ][cluster.service          ] [Shriker] detected_master [the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]], added {[Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]])
[2015-10-14 16:21:25,462][INFO ][gateway.local.state.meta ] [Shriker] [events_development_20151014161811608] dangling index, exists on local file system, but not in cluster metadata, auto import to cluster state [YES]
[2015-10-14 16:21:25,522][INFO ][http                     ] [Shriker] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.145:9200]}
[2015-10-14 16:21:25,529][INFO ][node                     ] [Shriker] started
[2015-10-14 16:34:52,849][INFO ][cluster.service          ] [Shriker] removed {[Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],}, reason: zen-disco-receive(from master [[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]])
[2015-10-14 16:34:57,323][INFO ][cluster.service          ] [Shriker] added {[Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],}, reason: zen-disco-receive(from master [[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]])
[2015-10-14 16:42:54,892][WARN ][transport.netty          ] [Shriker] exception caught on transport layer [[id: 0x4c6330b1, /192.168.1.145:61173 => /192.168.1.164:9300]], closing connection
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-14 16:42:55,193][INFO ][discovery.zen            ] [Shriker] master_left [[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]], reason [transport disconnected]
[2015-10-14 16:42:55,201][WARN ][discovery.zen            ] [Shriker] master left (reason = transport disconnected), current nodes: {[Shriker][vU_nG2vfSC-sFsZb60bDGQ][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]],[Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],}
[2015-10-14 16:42:55,206][INFO ][cluster.service          ] [Shriker] removed {[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-master_failed ([the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]])
[2015-10-14 16:43:25,413][WARN ][cluster.service          ] [Shriker] cluster state update task [zen-disco-master_failed ([the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]])] took 30.2s above the warn threshold of 30s
[2015-10-14 16:43:25,414][INFO ][cluster.service          ] [Shriker] detected_master [Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]], reason: zen-disco-receive(from master [[Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-14 16:44:46,081][INFO ][cluster.service          ] [Shriker] added {[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-14 16:46:54,774][INFO ][cluster.service          ] [Shriker] removed {[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-14 16:46:54,820][WARN ][action.index             ] [Shriker] failed to perform indices:data/write/index on remote replica [the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]][events][2]
org.elasticsearch.transport.NodeDisconnectedException: [the Renegade Watcher Aron][inet[/192.168.1.164:9300]][indices:data/write/index[r]] disconnected
[2015-10-14 16:49:48,047][INFO ][cluster.service          ] [Shriker] added {[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-14 17:28:16,508][WARN ][transport.netty          ] [Shriker] exception caught on transport layer [[id: 0x25c21fd8, /192.168.1.145:51561 => /192.168.1.164:9300]], closing connection
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-14 17:28:16,508][WARN ][transport.netty          ] [Shriker] exception caught on transport layer [[id: 0x77e16f82, /192.168.1.145:51562 => /192.168.1.164:9300]], closing connection
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-14 17:28:16,508][WARN ][transport.netty          ] [Shriker] exception caught on transport layer [[id: 0xf6d33846, /192.168.1.145:51563 => /192.168.1.164:9300]], closing connection
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-14 17:28:16,508][WARN ][transport.netty          ] [Shriker] exception caught on transport layer [[id: 0xe74286fc, /192.168.1.145:51565 => /192.168.1.164:9300]], closing connection
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-14 17:28:16,508][WARN ][transport.netty          ] [Shriker] exception caught on transport layer [[id: 0x1a9bf759, /192.168.1.145:51564 => /192.168.1.164:9300]], closing connection
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-14 17:28:55,948][INFO ][cluster.service          ] [Shriker] removed {[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-14 17:32:32,940][INFO ][cluster.service          ] [Shriker] added {[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-14 17:49:56,606][INFO ][cluster.service          ] [Shriker] removed {[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-14 18:04:03,062][INFO ][cluster.service          ] [Shriker] added {[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-14 18:34:10,739][INFO ][discovery.zen            ] [Shriker] master_left [[Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]], reason [transport disconnected]
[2015-10-14 18:34:10,739][INFO ][discovery.zen            ] [Shriker] master_left [[Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]], reason [shut_down]
[2015-10-14 18:34:10,793][WARN ][discovery.zen            ] [Shriker] master left (reason = shut_down), current nodes: {[Shriker][vU_nG2vfSC-sFsZb60bDGQ][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]],[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}
[2015-10-14 18:34:10,802][INFO ][cluster.service          ] [Shriker] removed {[Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],}, reason: zen-disco-master_failed ([Hermod][ro7b4KfYTpKXH8UTfKiwIg][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]])
[2015-10-14 18:34:13,974][INFO ][cluster.service          ] [Shriker] detected_master [Torrent][JnXiZrIqQuuyC3kmLYfNaQ][Beepo-Retina.local][inet[/192.168.1.8:9300]], added {[Torrent][JnXiZrIqQuuyC3kmLYfNaQ][Beepo-Retina.local][inet[/192.168.1.8:9300]],}, reason: zen-disco-receive(from master [[Torrent][JnXiZrIqQuuyC3kmLYfNaQ][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-14 20:53:59,219][INFO ][node                     ] [Danger] version[1.7.1], pid[11265], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-14 20:53:59,220][INFO ][node                     ] [Danger] initializing ...
[2015-10-14 20:53:59,395][INFO ][plugins                  ] [Danger] loaded [], sites []
[2015-10-14 20:53:59,478][INFO ][env                      ] [Danger] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [133.1gb], net total_space [232.6gb], types [hfs]
[2015-10-14 20:54:05,109][INFO ][node                     ] [Danger] initialized
[2015-10-14 20:54:05,122][INFO ][node                     ] [Danger] starting ...
[2015-10-14 20:54:05,503][INFO ][transport                ] [Danger] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.145:9300]}
[2015-10-14 20:54:05,634][INFO ][discovery                ] [Danger] elasticsearch/ruyQPlLlTum_W-_UpF7a2A
[2015-10-14 20:54:08,916][INFO ][cluster.service          ] [Danger] detected_master [Torrent][JnXiZrIqQuuyC3kmLYfNaQ][Beepo-Retina.local][inet[/192.168.1.8:9300]], added {[Torrent][JnXiZrIqQuuyC3kmLYfNaQ][Beepo-Retina.local][inet[/192.168.1.8:9300]],[Allatou][iO3X70YkRxGcqxwdnKdFxw][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],}, reason: zen-disco-receive(from master [[Torrent][JnXiZrIqQuuyC3kmLYfNaQ][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-14 20:54:08,993][INFO ][http                     ] [Danger] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.145:9200]}
[2015-10-14 20:54:08,994][INFO ][node                     ] [Danger] started
[2015-10-14 20:55:23,270][DEBUG][action.index             ] [Danger] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-10-14 20:55:33,277][DEBUG][action.index             ] [Danger] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-10-14 21:01:18,027][INFO ][cluster.service          ] [Danger] added {[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[Torrent][JnXiZrIqQuuyC3kmLYfNaQ][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-14 21:03:22,474][INFO ][discovery.zen            ] [Danger] master_left [[Torrent][JnXiZrIqQuuyC3kmLYfNaQ][Beepo-Retina.local][inet[/192.168.1.8:9300]]], reason [shut_down]
[2015-10-14 21:03:22,518][WARN ][discovery.zen            ] [Danger] master left (reason = shut_down), current nodes: {[Danger][ruyQPlLlTum_W-_UpF7a2A][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]],[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],[Allatou][iO3X70YkRxGcqxwdnKdFxw][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],}
[2015-10-14 21:03:22,523][INFO ][cluster.service          ] [Danger] removed {[Torrent][JnXiZrIqQuuyC3kmLYfNaQ][Beepo-Retina.local][inet[/192.168.1.8:9300]],}, reason: zen-disco-master_failed ([Torrent][JnXiZrIqQuuyC3kmLYfNaQ][Beepo-Retina.local][inet[/192.168.1.8:9300]])
[2015-10-14 21:03:25,768][INFO ][cluster.service          ] [Danger] detected_master [Allatou][iO3X70YkRxGcqxwdnKdFxw][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]], reason: zen-disco-receive(from master [[Allatou][iO3X70YkRxGcqxwdnKdFxw][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-14 21:24:20,815][DEBUG][action.index             ] [Danger] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-10-14 21:24:30,828][DEBUG][action.index             ] [Danger] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-10-14 21:25:03,767][WARN ][transport.netty          ] [Danger] exception caught on transport layer [[id: 0xc53aca41, /192.168.1.145:63258 => /192.168.1.126:9300]], closing connection
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-14 21:25:03,884][INFO ][discovery.zen            ] [Danger] master_left [[Allatou][iO3X70YkRxGcqxwdnKdFxw][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]], reason [transport disconnected]
[2015-10-14 21:25:03,885][WARN ][discovery.zen            ] [Danger] master left (reason = transport disconnected), current nodes: {[Danger][ruyQPlLlTum_W-_UpF7a2A][Madisons-MacBook-Air.local][inet[/192.168.1.145:9300]],[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}
[2015-10-14 21:25:03,886][INFO ][cluster.service          ] [Danger] removed {[Allatou][iO3X70YkRxGcqxwdnKdFxw][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],}, reason: zen-disco-master_failed ([Allatou][iO3X70YkRxGcqxwdnKdFxw][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]])
[2015-10-14 21:25:07,474][INFO ][cluster.service          ] [Danger] detected_master [Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]], added {[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]],}, reason: zen-disco-receive(from master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-14 21:26:48,984][INFO ][cluster.service          ] [Danger] removed {[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-14 21:27:54,268][INFO ][cluster.service          ] [Danger] added {[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-14 21:32:50,677][INFO ][cluster.service          ] [Danger] removed {[the Renegade Watcher Aron][oBzrVcBuT6KjOrdIJLrAXw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-14 21:37:19,076][INFO ][cluster.service          ] [Danger] added {[Kraven the Hunter][SdqJ8RO3Te6hJ5PSXRkRKw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-14 22:13:36,094][INFO ][cluster.service          ] [Danger] removed {[Kraven the Hunter][SdqJ8RO3Te6hJ5PSXRkRKw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-14 22:16:30,777][INFO ][cluster.service          ] [Danger] added {[Kraven the Hunter][SdqJ8RO3Te6hJ5PSXRkRKw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-14 23:01:36,618][INFO ][cluster.service          ] [Danger] removed {[Kraven the Hunter][SdqJ8RO3Te6hJ5PSXRkRKw][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[Vindaloo][if7uzR_tRZi6vnJyi8EU0w][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
>>>>>>> developer
[2015-10-15 14:14:49,760][INFO ][node                     ] [Patriot] version[1.7.1], pid[20492], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-15 14:14:49,760][INFO ][node                     ] [Patriot] initializing ...
[2015-10-15 14:14:49,887][INFO ][plugins                  ] [Patriot] loaded [], sites []
[2015-10-15 14:14:49,944][INFO ][env                      ] [Patriot] using [1] data paths, mounts [[/ (/dev/disk0s2)]], net usable_space [26.1gb], net total_space [352.9gb], types [hfs]
[2015-10-15 14:14:52,664][INFO ][node                     ] [Patriot] initialized
[2015-10-15 14:14:52,665][INFO ][node                     ] [Patriot] starting ...
[2015-10-15 14:14:52,761][INFO ][transport                ] [Patriot] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.8:9300]}
[2015-10-15 14:14:52,786][INFO ][discovery                ] [Patriot] elasticsearch/VTJ3zgtBSqmGpvshyOEPhQ
[2015-10-15 14:14:56,574][INFO ][cluster.service          ] [Patriot] new_master [Patriot][VTJ3zgtBSqmGpvshyOEPhQ][Beepo-Retina.local][inet[/192.168.1.8:9300]], reason: zen-disco-join (elected_as_master)
[2015-10-15 14:14:56,599][INFO ][http                     ] [Patriot] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.8:9200]}
[2015-10-15 14:14:56,599][INFO ][node                     ] [Patriot] started
[2015-10-15 14:14:56,634][INFO ][gateway                  ] [Patriot] recovered [2] indices into cluster_state
[2015-10-15 14:15:26,588][WARN ][cluster.routing.allocation.decider] [Patriot] high disk watermark [90%] exceeded on [VTJ3zgtBSqmGpvshyOEPhQ][Patriot] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 14:15:26,588][INFO ][cluster.routing.allocation.decider] [Patriot] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:15:56,584][WARN ][cluster.routing.allocation.decider] [Patriot] high disk watermark [90%] exceeded on [VTJ3zgtBSqmGpvshyOEPhQ][Patriot] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 14:16:26,581][WARN ][cluster.routing.allocation.decider] [Patriot] high disk watermark [90%] exceeded on [VTJ3zgtBSqmGpvshyOEPhQ][Patriot] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 14:16:56,589][WARN ][cluster.routing.allocation.decider] [Patriot] high disk watermark [90%] exceeded on [VTJ3zgtBSqmGpvshyOEPhQ][Patriot] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 14:16:56,589][INFO ][cluster.routing.allocation.decider] [Patriot] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:17:26,586][WARN ][cluster.routing.allocation.decider] [Patriot] high disk watermark [90%] exceeded on [VTJ3zgtBSqmGpvshyOEPhQ][Patriot] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 14:17:56,588][WARN ][cluster.routing.allocation.decider] [Patriot] high disk watermark [90%] exceeded on [VTJ3zgtBSqmGpvshyOEPhQ][Patriot] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 14:17:56,588][INFO ][cluster.routing.allocation.decider] [Patriot] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:18:26,592][WARN ][cluster.routing.allocation.decider] [Patriot] high disk watermark [90%] exceeded on [VTJ3zgtBSqmGpvshyOEPhQ][Patriot] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 14:18:56,592][WARN ][cluster.routing.allocation.decider] [Patriot] high disk watermark [90%] exceeded on [VTJ3zgtBSqmGpvshyOEPhQ][Patriot] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:18:56,592][INFO ][cluster.routing.allocation.decider] [Patriot] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:19:16,803][INFO ][node                     ] [Patriot] stopping ...
[2015-10-15 14:19:16,845][INFO ][node                     ] [Patriot] stopped
[2015-10-15 14:19:16,846][INFO ][node                     ] [Patriot] closing ...
[2015-10-15 14:19:16,851][INFO ][node                     ] [Patriot] closed
>>>>>>> developer
[2015-10-15 14:32:45,072][INFO ][node                     ] [Doctor Doom] version[1.7.1], pid[22510], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-15 14:32:45,072][INFO ][node                     ] [Doctor Doom] initializing ...
[2015-10-15 14:32:45,141][INFO ][plugins                  ] [Doctor Doom] loaded [], sites []
[2015-10-15 14:32:45,179][INFO ][env                      ] [Doctor Doom] using [1] data paths, mounts [[/ (/dev/disk0s2)]], net usable_space [26.1gb], net total_space [352.9gb], types [hfs]
[2015-10-15 14:32:47,191][INFO ][node                     ] [Doctor Doom] initialized
[2015-10-15 14:32:47,191][INFO ][node                     ] [Doctor Doom] starting ...
[2015-10-15 14:32:47,256][INFO ][transport                ] [Doctor Doom] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.8:9300]}
[2015-10-15 14:32:47,272][INFO ][discovery                ] [Doctor Doom] elasticsearch/1ulySXLlQnKtQTyYaJsMAA
[2015-10-15 14:32:51,048][INFO ][cluster.service          ] [Doctor Doom] new_master [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]], reason: zen-disco-join (elected_as_master)
[2015-10-15 14:32:51,083][INFO ][http                     ] [Doctor Doom] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.8:9200]}
[2015-10-15 14:32:51,083][INFO ][node                     ] [Doctor Doom] started
[2015-10-15 14:32:51,106][INFO ][gateway                  ] [Doctor Doom] recovered [2] indices into cluster_state
[2015-10-15 14:33:21,061][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 14:33:21,061][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:33:51,056][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 14:33:59,790][INFO ][cluster.metadata         ] [Doctor Doom] [events_development_20151014220818574] update_mapping [event] (dynamic)
[2015-10-15 14:34:21,057][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 14:34:51,057][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 14:34:51,057][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:35:21,058][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:35:51,057][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:35:51,057][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:36:06,319][INFO ][cluster.service          ] [Doctor Doom] added {[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(join from node[[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]])
[2015-10-15 14:36:06,371][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:36:21,061][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:36:51,061][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:36:51,061][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:37:21,213][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.8gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:37:51,360][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.8gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:37:51,361][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:38:21,111][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.8gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:38:51,089][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.8gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:39:21,144][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.8gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:39:21,145][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:39:51,244][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.8gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:40:21,070][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.8gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:40:51,145][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.8gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:40:51,146][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:41:21,072][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.8gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:41:51,548][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.8gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:41:51,548][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:42:21,153][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.8gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:42:51,084][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:43:21,186][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:43:21,186][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:43:51,120][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:44:21,501][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:44:21,502][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:44:51,195][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:45:21,298][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:45:51,096][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:45:51,097][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:46:21,214][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:46:51,309][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:46:51,309][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:47:21,091][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:47:51,111][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:48:21,109][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:48:21,109][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:49:06,091][DEBUG][action.admin.cluster.node.stats] [Doctor Doom] failed to execute on node [By6iy8QiQG2RvXe_GP6aaA]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [Captain Britain][inet[/192.168.1.164:9300]][cluster:monitor/nodes/stats[n]] request_id [1348] timed out after [15005ms]
	at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-15 14:49:21,090][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 14:49:36,090][DEBUG][action.admin.cluster.node.stats] [Doctor Doom] failed to execute on node [By6iy8QiQG2RvXe_GP6aaA]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [Captain Britain][inet[/192.168.1.164:9300]][cluster:monitor/nodes/stats[n]] request_id [1360] timed out after [15002ms]
	at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-15 14:49:37,905][WARN ][transport.netty          ] [Doctor Doom] exception caught on transport layer [[id: 0xad69d8f6, /192.168.1.8:54319 => /192.168.1.164:9300]], closing connection
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-15 14:49:37,909][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][0], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@52d36421]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,909][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][2], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@52d36421]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,910][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@52d36421]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,910][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@52d36421]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,909][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@52d36421]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,909][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@52d36421]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,909][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][0], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@52d36421]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,911][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@31a82043]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,911][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@31a82043]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,911][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@31a82043]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,911][INFO ][cluster.service          ] [Doctor Doom] removed {[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-node_failed([Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]), reason transport disconnected
[2015-10-15 14:49:37,911][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@31a82043]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,911][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][2], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@31a82043]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,911][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][2], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@31a82043]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,911][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@31a82043]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,910][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][0], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@31a82043]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,910][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@31a82043]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,910][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][0], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@31a82043]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,910][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@52d36421]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,910][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@52d36421]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,910][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][2], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@52d36421]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 14:49:37,918][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [8] unassigned shards, next check in [59.9s]
[2015-10-15 14:49:37,922][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 14:49:37,923][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:49:51,093][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:50:21,092][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:50:51,094][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:50:51,094][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:51:21,098][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:51:51,098][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:51:51,098][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:52:21,102][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:52:51,105][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:52:51,105][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:53:21,107][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:53:51,108][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:53:51,108][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:54:21,112][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:54:51,113][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:54:51,113][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:55:21,112][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:55:51,113][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:55:51,113][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:56:21,113][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:56:37,436][INFO ][cluster.service          ] [Doctor Doom] added {[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(join from node[[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]])
[2015-10-15 14:56:37,475][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 14:56:37,484][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:56:51,118][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:56:51,118][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:56:51,302][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 14:56:51,419][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 14:56:51,814][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 14:56:52,053][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 14:56:52,580][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 14:56:52,687][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 14:56:52,725][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 14:57:21,192][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:57:51,281][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:57:51,281][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:58:21,427][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:58:51,195][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:59:21,305][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 14:59:21,305][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 14:59:51,568][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:00:21,147][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:00:51,130][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:00:51,130][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:01:21,234][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:01:51,190][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:01:51,190][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:02:21,139][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:02:51,408][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:02:51,408][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:03:21,211][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:03:33,076][INFO ][cluster.service          ] [Doctor Doom] added {[G-Force][WXpxV3arQ7KsT6nURFe7ig][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],}, reason: zen-disco-receive(join from node[[G-Force][WXpxV3arQ7KsT6nURFe7ig][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-15 15:03:33,223][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.4gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:03:33,223][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 25.9gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:03:51,151][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.4gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:03:51,151][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:04:21,478][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:04:21,478][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:04:21,478][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:04:51,230][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:04:51,230][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:05:21,156][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:05:21,156][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:05:51,458][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:05:51,458][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:05:51,458][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:06:21,232][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:06:21,232][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:06:51,318][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:06:51,318][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:07:21,616][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:07:21,616][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:07:21,616][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:07:51,220][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:07:51,220][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:08:21,355][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:08:21,355][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:08:51,168][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:08:51,169][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:08:51,169][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:09:21,266][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:09:21,267][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:09:51,363][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:09:51,363][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:09:51,363][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:10:21,203][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:10:21,203][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:10:51,274][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:10:51,274][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:11:21,178][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:11:21,178][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:11:21,178][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:11:51,373][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:11:51,373][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:12:21,277][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:12:21,278][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:12:21,278][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:12:51,176][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:12:51,176][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:13:21,460][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:13:21,460][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:13:21,460][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:13:51,286][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:13:51,287][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:14:21,567][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:14:21,567][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:14:21,567][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:14:51,188][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:14:51,188][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:15:21,191][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:15:21,192][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:16:06,178][DEBUG][action.admin.cluster.node.stats] [Doctor Doom] failed to execute on node [By6iy8QiQG2RvXe_GP6aaA]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [Captain Britain][inet[/192.168.1.164:9300]][cluster:monitor/nodes/stats[n]] request_id [4066] timed out after [15005ms]
	at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-15 15:16:21,178][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:16:21,178][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:16:21,178][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:16:36,176][DEBUG][action.admin.cluster.node.stats] [Doctor Doom] failed to execute on node [By6iy8QiQG2RvXe_GP6aaA]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [Captain Britain][inet[/192.168.1.164:9300]][cluster:monitor/nodes/stats[n]] request_id [4108] timed out after [15004ms]
	at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-15 15:16:51,179][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:16:51,179][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:16:51,179][WARN ][discovery.zen.publish    ] [Doctor Doom] timed out waiting for all nodes to process published state [74] (timeout [30s], pending nodes: [[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]])
[2015-10-15 15:16:51,180][WARN ][cluster.service          ] [Doctor Doom] cluster state update task [cluster_reroute (api)] took 30s above the warn threshold of 30s
[2015-10-15 15:17:02,344][WARN ][transport                ] [Doctor Doom] Received response for a request that has timed out, sent [41172ms] ago, timed out [26168ms] ago, action [cluster:monitor/nodes/stats[n]], node [[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]], id [4108]
[2015-10-15 15:17:04,884][INFO ][cluster.service          ] [Doctor Doom] removed {[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-node_failed([Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]), reason failed to ping, tried [3] times, each with maximum [30s] timeout
[2015-10-15 15:17:05,191][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [8] unassigned shards, next check in [59.6s]
[2015-10-15 15:17:05,199][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][0], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@5c3d5718]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,199][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@5c3d5718]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,199][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@809c62d]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,199][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][0], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@5c3d5718]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,200][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@5c3d5718]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,199][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@5c3d5718]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,199][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][2], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@5c3d5718]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,199][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@5c3d5718]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,200][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@2f1ad5d]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,200][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@2f1ad5d]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,200][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@2f1ad5d]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,202][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@809c62d]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,200][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@2f1ad5d]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,200][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][2], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@2f1ad5d]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,200][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][0], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@2f1ad5d]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,200][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@2f1ad5d]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,202][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][2], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@809c62d]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,202][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][2], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@809c62d]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,201][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][0], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@809c62d]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,201][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@809c62d]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:17:05,201][DEBUG][action.admin.cluster.node.stats] [Doctor Doom] failed to execute on node [By6iy8QiQG2RvXe_GP6aaA]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][cluster:monitor/nodes/stats[n]] disconnected
[2015-10-15 15:17:05,204][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:17:05,204][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:17:08,838][INFO ][cluster.service          ] [Doctor Doom] added {[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(join from node[[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]])
[2015-10-15 15:17:08,884][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:17:08,884][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:17:21,480][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:17:21,480][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:17:21,480][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:17:51,202][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:17:51,202][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:18:21,462][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:18:21,462][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:18:51,210][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:18:51,210][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:18:51,210][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:19:21,498][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:19:21,498][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:19:51,325][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:19:51,325][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:19:51,325][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:20:21,713][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:20:21,713][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:20:51,224][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:20:51,225][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:21:21,319][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:21:21,319][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:21:21,319][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:21:51,220][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:21:51,221][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:22:21,234][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:22:21,234][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:22:51,430][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:22:51,430][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:22:51,430][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:23:21,484][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:23:21,484][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:23:51,240][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:23:51,240][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:24:21,279][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:24:21,279][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:24:21,279][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:24:51,397][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:24:51,397][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:25:21,236][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:25:21,236][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:25:51,361][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:25:51,361][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:25:51,361][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:26:21,529][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:26:21,529][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:26:51,261][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:26:51,261][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:27:21,803][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:27:21,803][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:27:21,803][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:27:51,288][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:27:51,288][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:28:21,270][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:28:21,270][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:28:51,659][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:28:51,660][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:28:51,660][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:29:21,251][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:29:21,251][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:29:51,408][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:29:51,408][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:30:21,507][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:30:21,507][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:30:21,507][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:30:51,266][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:30:51,266][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:31:21,417][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:31:21,417][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:31:51,416][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:31:51,416][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:31:51,416][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:32:21,319][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:32:21,319][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:32:51,426][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:32:51,426][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:32:51,427][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:33:21,277][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:33:21,277][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:33:51,328][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:33:51,328][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:34:21,434][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:34:21,434][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:34:21,434][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:34:51,674][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:34:51,674][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:35:21,979][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:35:21,979][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:35:21,979][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:35:51,342][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:35:51,342][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:36:21,444][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:36:21,444][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:36:51,286][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:36:51,286][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:36:51,286][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:37:21,361][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:37:21,361][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:38:06,287][DEBUG][action.admin.cluster.node.stats] [Doctor Doom] failed to execute on node [By6iy8QiQG2RvXe_GP6aaA]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [Captain Britain][inet[/192.168.1.164:9300]][cluster:monitor/nodes/stats[n]] request_id [7259] timed out after [15003ms]
	at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-15 15:38:21,287][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:38:21,287][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:38:21,287][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:38:36,285][DEBUG][action.admin.cluster.node.stats] [Doctor Doom] failed to execute on node [By6iy8QiQG2RvXe_GP6aaA]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [Captain Britain][inet[/192.168.1.164:9300]][cluster:monitor/nodes/stats[n]] request_id [7301] timed out after [14999ms]
	at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-15 15:38:51,287][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:38:51,288][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:38:51,288][WARN ][discovery.zen.publish    ] [Doctor Doom] timed out waiting for all nodes to process published state [105] (timeout [30s], pending nodes: [[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]])
[2015-10-15 15:38:51,290][WARN ][cluster.service          ] [Doctor Doom] cluster state update task [cluster_reroute (api)] took 30s above the warn threshold of 30s
[2015-10-15 15:38:56,547][WARN ][transport.netty          ] [Doctor Doom] exception caught on transport layer [[id: 0x3733be92, /192.168.1.8:55612 => /192.168.1.164:9300]], closing connection
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-15 15:38:56,549][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][0], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@568e034b]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,549][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][0], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@568e034b]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,549][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][2], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@568e034b]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,549][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][2], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@568e034b]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,549][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@568e034b]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,550][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@568e034b]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,550][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@1974edd1]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,549][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@568e034b]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,551][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@1974edd1]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,549][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@568e034b]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,549][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@568e034b]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,549][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@568e034b]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,552][DEBUG][action.admin.cluster.node.stats] [Doctor Doom] failed to execute on node [By6iy8QiQG2RvXe_GP6aaA]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][cluster:monitor/nodes/stats[n]] disconnected
[2015-10-15 15:38:56,551][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@1974edd1]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,553][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][2], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@35199d7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,553][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@35199d7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,551][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@1974edd1]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,551][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@1974edd1]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,551][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][2], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@1974edd1]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,551][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][2], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@1974edd1]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,550][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][0], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@1974edd1]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,550][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@1974edd1]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,550][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][0], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@1974edd1]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,554][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][2], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@35199d7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,554][INFO ][cluster.service          ] [Doctor Doom] removed {[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-node_failed([Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]), reason transport disconnected
[2015-10-15 15:38:56,554][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@35199d7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,554][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@35199d7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,553][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@35199d7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,553][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@35199d7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,553][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@35199d7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,553][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][0], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@35199d7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,553][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][0], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@35199d7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 15:38:56,562][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:38:56,562][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:38:56,571][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [8] unassigned shards, next check in [59.9s]
[2015-10-15 15:39:21,650][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:39:21,650][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:39:21,650][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:39:51,834][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:39:51,834][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:40:22,551][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:40:22,551][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:40:22,551][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:40:51,300][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:40:51,300][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:41:21,583][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:41:21,583][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:41:51,340][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:41:51,340][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:41:51,340][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:42:21,302][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:42:21,302][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:42:51,306][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:42:51,306][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:43:21,390][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:43:21,390][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:43:21,390][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:43:51,534][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:43:51,534][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:44:21,308][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:44:21,309][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:44:51,311][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:44:51,311][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:44:51,312][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:45:21,313][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:45:21,314][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:45:51,694][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:45:51,694][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:45:51,694][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:46:21,409][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:46:21,409][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:46:51,320][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:46:51,320][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:47:21,873][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:47:21,873][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:47:21,873][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:47:51,324][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:47:51,324][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:48:21,325][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:48:21,325][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:48:51,357][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:48:51,357][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:48:51,357][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:49:21,425][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:49:21,425][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:49:30,525][INFO ][cluster.service          ] [Doctor Doom] added {[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(join from node[[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]])
[2015-10-15 15:49:30,546][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 15:49:30,552][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:49:30,552][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:49:51,328][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.4gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:49:51,328][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.2gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:50:21,328][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.4gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:50:21,328][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:50:21,328][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:50:21,667][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 15:50:22,019][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 15:50:22,378][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 15:50:22,657][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 15:50:22,871][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 15:50:23,270][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 15:50:23,564][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 15:50:51,344][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.4gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:50:51,344][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:51:21,519][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:51:21,519][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:51:21,519][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:51:51,467][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:51:51,467][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:52:22,050][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:52:22,050][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:52:22,050][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:52:51,350][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:52:51,350][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:53:21,489][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:53:21,489][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:53:51,360][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:53:51,360][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:53:51,360][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:54:21,378][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:54:21,378][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:54:51,788][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:54:51,788][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 15:54:51,788][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:55:21,596][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:55:21,596][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:55:51,360][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:55:51,360][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:56:21,493][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:56:21,493][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:56:21,493][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:56:52,073][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:56:52,073][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:57:21,401][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:57:21,401][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:57:51,506][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:57:51,506][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:57:51,506][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:58:21,610][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:58:21,610][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:58:51,362][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:58:51,362][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:59:21,674][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:59:21,674][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 15:59:21,674][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 15:59:51,731][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 15:59:51,731][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:00:21,394][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.3gb[1.4%], shards will be relocated away from this node
[2015-10-15 16:00:21,394][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:00:51,523][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.4%], shards will be relocated away from this node
[2015-10-15 16:00:51,523][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:00:51,523][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:01:21,373][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.4%], shards will be relocated away from this node
[2015-10-15 16:01:21,373][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:01:51,490][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.4%], shards will be relocated away from this node
[2015-10-15 16:01:51,490][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:02:21,380][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.4%], shards will be relocated away from this node
[2015-10-15 16:02:21,380][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:02:21,380][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:02:51,463][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.4%], shards will be relocated away from this node
[2015-10-15 16:02:51,464][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:03:21,562][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.4%], shards will be relocated away from this node
[2015-10-15 16:03:21,562][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:03:21,562][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:03:51,389][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.4%], shards will be relocated away from this node
[2015-10-15 16:03:51,389][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:04:21,596][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.4%], shards will be relocated away from this node
[2015-10-15 16:04:21,596][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:04:21,596][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:04:51,604][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.4%], shards will be relocated away from this node
[2015-10-15 16:04:51,604][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:05:21,584][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.4%], shards will be relocated away from this node
[2015-10-15 16:05:21,584][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:05:51,620][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.4%], shards will be relocated away from this node
[2015-10-15 16:05:51,621][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:05:51,621][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:17:39,152][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:17:39,152][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:18:07,830][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:18:07,830][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:18:37,896][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:18:37,896][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:18:37,896][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:24:57,514][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:24:57,514][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:25:26,274][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:25:26,274][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:26:11,265][DEBUG][action.admin.cluster.node.stats] [Doctor Doom] failed to execute on node [By6iy8QiQG2RvXe_GP6aaA]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [Captain Britain][inet[/192.168.1.164:9300]][cluster:monitor/nodes/stats[n]] request_id [10928] timed out after [15001ms]
	at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-15 16:26:26,264][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:26:26,265][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:26:26,265][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:26:34,624][WARN ][transport.netty          ] [Doctor Doom] exception caught on transport layer [[id: 0x1ab15679, /192.168.1.8:55998 => /192.168.1.164:9300]], closing connection
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-15 16:26:34,626][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][0], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@7a7754b7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,626][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][0], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@7a7754b7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,627][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][2], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@7a7754b7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,626][DEBUG][action.admin.cluster.node.stats] [Doctor Doom] failed to execute on node [By6iy8QiQG2RvXe_GP6aaA]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][cluster:monitor/nodes/stats[n]] disconnected
[2015-10-15 16:26:34,627][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][0], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@bc90a51]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,627][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][0], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@bc90a51]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,627][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@bc90a51]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,627][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@7a7754b7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,627][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@7a7754b7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,627][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][2], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@7a7754b7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,627][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@7a7754b7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,627][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@7a7754b7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,627][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@7a7754b7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,627][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@7a7754b7]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,628][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@bc90a51]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,628][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@bc90a51]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,628][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@bc90a51]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,628][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@bc90a51]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,627][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][2], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@bc90a51]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,627][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][2], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@bc90a51]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,627][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@bc90a51]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:26:34,633][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:26:34,634][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:26:34,636][INFO ][cluster.service          ] [Doctor Doom] removed {[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-node_failed([Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]), reason transport disconnected
[2015-10-15 16:26:34,651][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [8] unassigned shards, next check in [59.9s]
[2015-10-15 16:26:56,272][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:26:56,272][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:27:26,333][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:27:26,333][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:27:26,333][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:27:28,494][INFO ][cluster.service          ] [Doctor Doom] added {[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(join from node[[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]])
[2015-10-15 16:27:28,534][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:27:28,534][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:27:34,856][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:34,874][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:35,411][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[INITIALIZING], unassigned_info[[reason=NODE_LEFT], at[2015-10-15T23:26:34.628Z], details[node_left[By6iy8QiQG2RvXe_GP6aaA]]], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:35,430][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:35,571][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:35,626][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:36,145][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[INITIALIZING], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-10-15T23:27:35.412Z], details[shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:36,347][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:36,370][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:36,629][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[INITIALIZING], unassigned_info[[reason=NODE_LEFT], at[2015-10-15T23:26:34.628Z], details[node_left[By6iy8QiQG2RvXe_GP6aaA]]], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:36,654][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:36,693][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:36,909][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:37,279][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[INITIALIZING], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-10-15T23:27:36.351Z], details[shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:37,294][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:37,459][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:37,878][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:37,985][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:38,183][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:38,200][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:38,800][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:38,801][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:38,811][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:38,826][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:38,826][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:38,842][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:38,855][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:38,885][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:38,902][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:38,928][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:38,939][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:39,629][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:39,630][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:39,641][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:39,671][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:39,686][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:39,710][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:39,720][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:39,741][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:39,755][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:40,191][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:40,285][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:40,331][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:40,351][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:40,373][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:40,383][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:40,385][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:40,400][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:40,948][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:40,949][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:40,960][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:40,976][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:40,977][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:40,990][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:41,003][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:41,025][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:41,035][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:41,540][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:41,568][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:41,629][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:41,643][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:41,658][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:41,668][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:41,669][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:41,683][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:41,696][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:41,708][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:41,729][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:41,738][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:42,484][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:42,485][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:42,505][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:42,647][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:42,661][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:42,677][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:42,688][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:43,201][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:43,213][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:43,218][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:43,242][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:43,247][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:43,277][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:43,750][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:43,763][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:43,785][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:43,794][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:43,812][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:43,823][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:43,846][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:43,856][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:43,878][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:43,888][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:44,635][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:44,636][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:44,645][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:44,654][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:44,663][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:44,686][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:44,705][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:44,714][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:44,736][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:44,748][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:45,179][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:45,190][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:45,286][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:45,297][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:45,868][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:45,869][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:45,878][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:45,892][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:45,911][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:45,922][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:45,942][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:45,951][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:45,970][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:45,979][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:46,010][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:46,019][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:46,041][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:46,049][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:46,067][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:46,077][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:46,097][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:46,107][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:46,474][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:46,485][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:46,638][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:46,649][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,088][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:47,100][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,181][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:47,191][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,212][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:47,222][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,242][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:47,254][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,275][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:47,286][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,641][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:47,697][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,718][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:47,727][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,756][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:47,765][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,787][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:47,797][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,813][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:47,820][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:47,823][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,834][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,848][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:47,857][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:47,859][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,869][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,884][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:47,894][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,913][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:47,921][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,942][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:47,951][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,970][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:47,978][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:47,998][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:48,008][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:48,028][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:48,037][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:48,057][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:48,067][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:48,085][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:48,094][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:48,626][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:48,627][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:48,643][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:48,719][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:48,734][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:48,748][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:48,759][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:48,759][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:48,776][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:48,795][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:48,808][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:48,828][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:48,836][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:49,295][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:49,305][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:49,327][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:49,338][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:49,365][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:49,374][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:49,395][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:49,407][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:49,426][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:49,436][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:50,161][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:50,162][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:50,178][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:50,398][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:50,465][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:50,479][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:50,491][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:50,518][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:50,528][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,081][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:51,082][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:51,092][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,102][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:51,105][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,122][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:51,132][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,165][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:51,181][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,200][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:51,210][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,231][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:51,241][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,261][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:51,269][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,289][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:51,297][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,318][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:51,328][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,352][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:51,362][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,382][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:51,392][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,412][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:51,421][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,441][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:51,450][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,470][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:51,479][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,498][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:51,508][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,533][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:51,544][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,676][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:51,686][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:51,706][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:51,716][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:52,173][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:52,182][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:52,206][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:52,223][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:52,244][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:52,250][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:52,255][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:52,267][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:52,268][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:52,291][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:52,930][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:52,932][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:52,942][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:52,956][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:52,958][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:52,982][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:53,009][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:53,020][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:53,044][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:53,053][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:53,076][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:53,085][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:53,106][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:53,117][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:53,137][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:53,148][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:53,167][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:53,176][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:53,479][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:53,538][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:53,562][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:53,581][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:53,607][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:53,616][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:53,636][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:53,646][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:53,665][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:53,676][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:53,698][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:53,700][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:53,711][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:53,728][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:54,256][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:54,272][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:54,464][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:54,478][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:54,995][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:55,008][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:55,074][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:55,087][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:55,099][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:55,114][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:55,134][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:55,145][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:55,165][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:55,175][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:55,631][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:55,689][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:55,705][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:55,720][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:55,720][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:55,743][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:56,245][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:56,255][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:56,279][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:27:56,279][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:27:56,281][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:56,290][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:56,315][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:56,324][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:56,794][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:56,802][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:56,824][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:56,832][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:56,859][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:56,869][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,372][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:57,381][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,397][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,402][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:57,406][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,417][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:57,417][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,429][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,440][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,462][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,471][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,490][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,500][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,534][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,544][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,566][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,575][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,591][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,600][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,618][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,627][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,645][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,654][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,672][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,683][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,702][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,711][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,731][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,740][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,758][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,767][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,788][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,796][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,815][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,824][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,843][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:57,852][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:57,977][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:57,987][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:58,008][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:58,016][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:58,034][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:58,042][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:58,059][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:58,068][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:58,086][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:58,096][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:58,116][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:58,124][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:58,145][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:58,154][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:58,173][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:58,182][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:58,385][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:58,395][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:58,417][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:58,427][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:58,448][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:58,456][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:58,477][WARN ][cluster.action.shard     ] [Doctor Doom] [events][1] received shard failed for [events][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [VsDJjKSvTl6Arm8h7b-nQA], reason [shard failure [failed recovery][RecoveryFailedException[[events][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events][1] from primary shard with sync id but number of docs differ: 21 (Doctor Doom, primary) vs 20(Captain Britain)]; ]]
[2015-10-15 16:27:58,486][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:27:58,714][WARN ][cluster.action.shard     ] [Doctor Doom] [events_development_20151014220818574][1] received shard failed for [events_development_20151014220818574][1], node[By6iy8QiQG2RvXe_GP6aaA], relocating [1ulySXLlQnKtQTyYaJsMAA], [P], s[INITIALIZING], indexUUID [6ucWIHSFQMeNtUaKEdYxEg], reason [shard failure [failed recovery][RecoveryFailedException[[events_development_20151014220818574][1]: Recovery failed from [Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]] into [Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]]; nested: RemoteTransportException[[Doctor Doom][inet[/192.168.1.8:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[events_development_20151014220818574][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[events_development_20151014220818574][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [events_development_20151014220818574][1] from primary shard with sync id but number of docs differ: 20 (Doctor Doom, primary) vs 19(Captain Britain)]; ]]
[2015-10-15 16:27:58,725][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:28:41,272][DEBUG][action.admin.cluster.node.stats] [Doctor Doom] failed to execute on node [By6iy8QiQG2RvXe_GP6aaA]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [Captain Britain][inet[/192.168.1.164:9300]][cluster:monitor/nodes/stats[n]] request_id [12330] timed out after [15004ms]
	at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-15 16:28:53,179][WARN ][transport.netty          ] [Doctor Doom] exception caught on transport layer [[id: 0xefbbcacc, /192.168.1.8:56715 => /192.168.1.164:9300]], closing connection
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-15 16:28:53,181][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][0], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@d73b68e]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:28:53,181][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][0], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@d73b68e]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:28:53,181][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][2], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@d73b68e]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:28:53,181][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@d73b68e]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:28:53,181][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][2], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@d73b68e]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:28:53,181][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][3], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@d73b68e]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:28:53,181][DEBUG][action.admin.indices.stats] [Doctor Doom] [events][4], node[By6iy8QiQG2RvXe_GP6aaA], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@d73b68e]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:28:53,181][DEBUG][action.admin.indices.stats] [Doctor Doom] [events_development_20151014220818574][4], node[By6iy8QiQG2RvXe_GP6aaA], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@d73b68e]
org.elasticsearch.transport.NodeDisconnectedException: [Captain Britain][inet[/192.168.1.164:9300]][indices:monitor/stats[s]] disconnected
[2015-10-15 16:28:53,184][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:28:53,184][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:28:53,184][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:28:53,184][INFO ][cluster.service          ] [Doctor Doom] removed {[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-node_failed([Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]), reason transport disconnected
[2015-10-15 16:28:53,202][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [6] unassigned shards, next check in [59.9s]
[2015-10-15 16:28:56,275][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:28:56,275][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:29:26,278][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:29:26,278][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:29:56,479][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:29:56,480][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:29:56,480][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:30:26,289][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:30:26,289][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:30:56,392][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:30:56,392][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:31:26,282][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:31:26,282][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:31:26,282][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:31:56,306][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:31:56,306][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:32:26,282][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:32:26,282][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:32:26,282][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:32:38,610][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:32:56,482][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:32:56,482][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:33:26,312][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:33:26,312][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:33:26,312][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:33:56,289][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:33:56,290][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:34:26,289][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:34:26,289][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:34:56,314][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:34:56,314][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:34:56,314][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:35:26,428][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:35:26,428][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:35:56,801][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:35:56,801][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:35:56,801][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:36:26,299][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:36:26,299][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:36:56,294][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:36:56,294][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:37:26,293][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:37:26,293][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:37:26,293][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:37:56,292][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:37:56,292][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:38:26,294][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:38:26,294][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:38:26,294][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:38:56,301][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:38:56,301][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:39:26,477][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:39:26,477][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:39:26,477][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:39:45,385][INFO ][cluster.service          ] [Doctor Doom] added {[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(join from node[[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]])
[2015-10-15 16:39:45,408][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:39:45,413][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:39:45,413][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:39:56,456][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:39:56,456][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:40:26,392][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:40:26,393][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:41:11,305][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:41:11,305][DEBUG][action.admin.cluster.node.stats] [Doctor Doom] failed to execute on node [By6iy8QiQG2RvXe_GP6aaA]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [Captain Britain][inet[/192.168.1.164:9300]][cluster:monitor/nodes/stats[n]] request_id [13156] timed out after [15001ms]
	at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-15 16:41:11,305][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:41:11,306][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:41:24,990][WARN ][transport.netty          ] [Doctor Doom] exception caught on transport layer [[id: 0xdcdb68c1, /192.168.1.8:56862 => /192.168.1.164:9300]], closing connection
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-15 16:41:24,995][INFO ][cluster.service          ] [Doctor Doom] removed {[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-node_failed([Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]), reason transport disconnected
[2015-10-15 16:41:25,073][INFO ][cluster.routing          ] [Doctor Doom] delaying allocation for [0] unassigned shards, next check in [0s]
[2015-10-15 16:41:26,309][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:41:26,309][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:41:56,392][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:41:56,392][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:42:26,811][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:42:26,811][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:42:26,811][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:42:56,604][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:42:56,604][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:43:26,403][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:43:26,403][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:43:56,320][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:43:56,320][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:43:56,320][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:44:26,318][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:44:26,318][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:44:56,319][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:44:56,319][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:44:56,319][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:45:26,521][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:45:26,521][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:45:56,500][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:45:56,500][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:45:56,500][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:46:26,424][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:46:26,424][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:46:56,496][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:46:56,496][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:47:26,768][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:47:26,768][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:47:26,768][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:47:56,348][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:47:56,348][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.4%], shards will be relocated away from this node
[2015-10-15 16:48:26,321][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:48:26,321][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:48:56,423][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:48:56,424][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:48:56,424][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:49:26,443][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:49:26,443][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:49:56,320][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:49:56,321][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:50:26,343][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:50:26,343][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:50:26,343][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:50:56,449][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:50:56,449][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:51:26,329][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:51:26,329][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:51:56,334][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:51:56,334][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:51:56,334][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:52:26,457][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:52:26,457][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:52:56,846][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:52:56,847][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:52:56,847][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:53:26,341][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:53:26,341][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:53:56,346][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:53:56,346][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:54:26,395][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:54:26,395][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:54:26,395][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 16:54:56,428][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:54:56,428][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:55:26,378][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:55:26,378][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:59:39,436][WARN ][discovery.zen            ] [Doctor Doom] received a request to rejoin the cluster from [WXpxV3arQ7KsT6nURFe7ig], current nodes: {[Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]],[G-Force][WXpxV3arQ7KsT6nURFe7ig][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],}
[2015-10-15 16:59:42,510][INFO ][cluster.service          ] [Doctor Doom] detected_master [G-Force][WXpxV3arQ7KsT6nURFe7ig][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]], reason: zen-disco-receive(from master [[G-Force][WXpxV3arQ7KsT6nURFe7ig][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-15 16:59:45,527][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [WXpxV3arQ7KsT6nURFe7ig][G-Force] free: 3.2gb[1.3%], shards will be relocated away from this node
[2015-10-15 16:59:45,527][WARN ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark [90%] exceeded on [1ulySXLlQnKtQTyYaJsMAA][Doctor Doom] free: 26.1gb[7.3%], shards will be relocated away from this node
[2015-10-15 16:59:45,527][INFO ][cluster.routing.allocation.decider] [Doctor Doom] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 17:08:39,335][INFO ][cluster.service          ] [Doctor Doom] added {[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[G-Force][WXpxV3arQ7KsT6nURFe7ig][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-15 17:15:16,165][WARN ][transport                ] [Doctor Doom] Received response for a request that has timed out, sent [350917ms] ago, timed out [17620ms] ago, action [internal:discovery/zen/fd/master_ping], node [[G-Force][WXpxV3arQ7KsT6nURFe7ig][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]], id [14602]
[2015-10-15 17:15:19,317][INFO ][cluster.service          ] [Doctor Doom] removed {[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]],}, reason: zen-disco-receive(from master [[G-Force][WXpxV3arQ7KsT6nURFe7ig][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-15 17:15:28,555][WARN ][transport                ] [Doctor Doom] Received response for a request that has timed out, sent [30009ms] ago, timed out [7ms] ago, action [internal:discovery/zen/fd/master_ping], node [[G-Force][WXpxV3arQ7KsT6nURFe7ig][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]], id [14603]
[2015-10-15 17:32:45,416][WARN ][transport.netty          ] [Doctor Doom] exception caught on transport layer [[id: 0x72b0d0f4, /192.168.1.8:55416 => /192.168.1.126:9300]], closing connection
java.io.IOException: Host is down
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:51)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:470)
	at org.elasticsearch.common.netty.channel.socket.nio.SocketSendBufferPool$UnpooledSendBuffer.transferTo(SocketSendBufferPool.java:203)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.write0(AbstractNioWorker.java:201)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.writeFromTaskLoop(AbstractNioWorker.java:151)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannel$WriteTask.run(AbstractNioChannel.java:315)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2015-10-15 17:32:45,418][INFO ][discovery.zen            ] [Doctor Doom] master_left [[G-Force][WXpxV3arQ7KsT6nURFe7ig][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]], reason [transport disconnected]
[2015-10-15 17:32:45,418][WARN ][discovery.zen            ] [Doctor Doom] master left (reason = transport disconnected), current nodes: {[Doctor Doom][1ulySXLlQnKtQTyYaJsMAA][Beepo-Retina.local][inet[/192.168.1.8:9300]],}
[2015-10-15 17:32:45,418][INFO ][cluster.service          ] [Doctor Doom] removed {[G-Force][WXpxV3arQ7KsT6nURFe7ig][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],}, reason: zen-disco-master_failed ([G-Force][WXpxV3arQ7KsT6nURFe7ig][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]])
[2015-10-15 17:33:34,721][INFO ][cluster.service          ] [Doctor Doom] detected_master [G-Force][WXpxV3arQ7KsT6nURFe7ig][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]], added {[G-Force][WXpxV3arQ7KsT6nURFe7ig][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],}, reason: zen-disco-receive(from master [[G-Force][WXpxV3arQ7KsT6nURFe7ig][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-15 17:33:35,046][WARN ][discovery.zen.ping.multicast] [Doctor Doom] received ping response ping_response{node [[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]], id[64], master [[Captain Britain][By6iy8QiQG2RvXe_GP6aaA][Joies-MacBook-Pro.local][inet[/192.168.1.164:9300]]], hasJoinedOnce [true], cluster_name[elasticsearch]} with no matching id [3]
[2015-10-15 17:46:50,007][INFO ][node                     ] [Doctor Doom] stopping ...
[2015-10-15 17:46:50,391][INFO ][node                     ] [Doctor Doom] stopped
[2015-10-15 17:46:50,391][INFO ][node                     ] [Doctor Doom] closing ...
[2015-10-15 17:46:50,395][INFO ][node                     ] [Doctor Doom] closed
[2015-10-15 19:34:48,197][INFO ][node                     ] [Kiwi Black] version[1.7.1], pid[1277], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-15 19:34:48,198][INFO ][node                     ] [Kiwi Black] initializing ...
[2015-10-15 19:34:48,507][INFO ][plugins                  ] [Kiwi Black] loaded [], sites []
[2015-10-15 19:34:48,609][INFO ][env                      ] [Kiwi Black] using [1] data paths, mounts [[/ (/dev/disk0s2)]], net usable_space [27.4gb], net total_space [352.9gb], types [hfs]
[2015-10-15 19:34:51,308][INFO ][node                     ] [Kiwi Black] initialized
[2015-10-15 19:34:51,308][INFO ][node                     ] [Kiwi Black] starting ...
[2015-10-15 19:34:51,408][INFO ][transport                ] [Kiwi Black] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.8:9300]}
[2015-10-15 19:34:51,432][INFO ][discovery                ] [Kiwi Black] elasticsearch/2jFkYAIWRHWLukCfmqA2Ug
[2015-10-15 19:34:55,210][INFO ][cluster.service          ] [Kiwi Black] new_master [Kiwi Black][2jFkYAIWRHWLukCfmqA2Ug][Beepo-Retina.local][inet[/192.168.1.8:9300]], reason: zen-disco-join (elected_as_master)
[2015-10-15 19:34:55,233][INFO ][http                     ] [Kiwi Black] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.8:9200]}
[2015-10-15 19:34:55,233][INFO ][node                     ] [Kiwi Black] started
[2015-10-15 19:34:55,276][INFO ][gateway                  ] [Kiwi Black] recovered [2] indices into cluster_state
[2015-10-15 19:35:25,223][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.2gb[7.7%], shards will be relocated away from this node
[2015-10-15 19:35:25,224][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:35:55,218][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.2gb[7.7%], shards will be relocated away from this node
[2015-10-15 19:36:25,215][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.2gb[7.7%], shards will be relocated away from this node
[2015-10-15 19:36:55,217][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.2gb[7.7%], shards will be relocated away from this node
[2015-10-15 19:36:55,217][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:37:25,219][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.2gb[7.7%], shards will be relocated away from this node
[2015-10-15 19:37:55,219][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.2gb[7.7%], shards will be relocated away from this node
[2015-10-15 19:37:55,219][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:38:25,218][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.1gb[7.7%], shards will be relocated away from this node
[2015-10-15 19:38:55,221][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.1gb[7.7%], shards will be relocated away from this node
[2015-10-15 19:38:55,221][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:39:25,220][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.1gb[7.7%], shards will be relocated away from this node
[2015-10-15 19:39:55,224][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.1gb[7.7%], shards will be relocated away from this node
[2015-10-15 19:39:55,224][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:40:25,223][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.1gb[7.7%], shards will be relocated away from this node
[2015-10-15 19:40:55,224][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.1gb[7.7%], shards will be relocated away from this node
[2015-10-15 19:40:55,224][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:41:25,224][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.1gb[7.7%], shards will be relocated away from this node
[2015-10-15 19:41:42,668][INFO ][cluster.service          ] [Kiwi Black] added {[Dominic Petros][Bd26-QbrSfiXdVuDlL1SsA][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],}, reason: zen-disco-receive(join from node[[Dominic Petros][Bd26-QbrSfiXdVuDlL1SsA][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-15 19:41:42,751][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [Bd26-QbrSfiXdVuDlL1SsA][Dominic Petros] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:41:42,751][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.1gb[7.7%], shards will be relocated away from this node
[2015-10-15 19:41:55,232][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [Bd26-QbrSfiXdVuDlL1SsA][Dominic Petros] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:41:55,232][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.1gb[7.7%], shards will be relocated away from this node
[2015-10-15 19:41:55,232][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:42:25,335][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [Bd26-QbrSfiXdVuDlL1SsA][Dominic Petros] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:42:25,335][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.1gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:42:55,267][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [Bd26-QbrSfiXdVuDlL1SsA][Dominic Petros] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:42:55,267][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.1gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:42:55,267][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:43:25,366][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [Bd26-QbrSfiXdVuDlL1SsA][Dominic Petros] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:43:25,366][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:43:46,557][INFO ][cluster.service          ] [Kiwi Black] removed {[Dominic Petros][Bd26-QbrSfiXdVuDlL1SsA][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],}, reason: zen-disco-node_left([Dominic Petros][Bd26-QbrSfiXdVuDlL1SsA][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]])
[2015-10-15 19:43:55,233][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:44:25,237][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:44:25,237][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:44:55,238][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:45:25,243][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:45:25,243][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:45:55,242][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:46:25,246][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:46:25,246][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:46:55,246][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:47:25,247][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:47:25,247][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:47:45,284][INFO ][cluster.service          ] [Kiwi Black] added {[Mister Sensitive][sT6msm-4TlC6PaXznxGPdQ][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]],}, reason: zen-disco-receive(join from node[[Mister Sensitive][sT6msm-4TlC6PaXznxGPdQ][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]])
[2015-10-15 19:47:45,352][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:47:45,352][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:47:55,392][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:47:55,393][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:48:25,253][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:48:25,253][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:48:25,253][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:48:55,253][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:48:55,253][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:49:25,254][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:49:25,254][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:49:25,254][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:49:55,259][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:49:55,259][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:50:25,304][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:50:25,304][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:50:25,304][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:50:55,408][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:50:55,408][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:51:25,386][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:51:25,386][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:51:25,386][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:51:55,312][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:51:55,312][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:52:25,277][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:52:25,277][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:52:55,319][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:52:55,319][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:52:55,319][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:53:25,285][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:53:25,285][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:53:55,434][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:53:55,434][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:53:55,434][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:54:25,531][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:54:25,532][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:54:55,336][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:54:55,336][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:55:25,437][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:55:25,437][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:55:25,438][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:55:55,328][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:55:55,328][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:56:25,352][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:56:25,352][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:56:55,296][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:56:55,296][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:56:55,296][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:57:25,295][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:57:25,295][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:57:55,349][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:57:55,349][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:57:55,349][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:58:25,302][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:58:25,302][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:58:55,602][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:58:55,602][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:58:55,602][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 19:59:25,605][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:59:25,605][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 19:59:55,463][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 19:59:55,464][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:00:25,307][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:00:25,307][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:00:25,307][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:00:55,368][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:00:55,368][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:01:25,309][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:01:25,309][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:01:25,310][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:01:55,884][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:01:55,884][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:02:25,312][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:02:25,313][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:02:25,313][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:02:55,486][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:02:55,486][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:03:25,317][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:03:25,318][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:03:25,318][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:03:55,317][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:03:55,317][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:04:25,321][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:04:25,321][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:04:25,321][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:04:55,381][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:04:55,381][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:05:25,325][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:05:25,325][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:05:25,325][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:05:55,327][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:05:55,327][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:06:25,326][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:06:25,326][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:06:25,326][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:06:55,337][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:06:55,337][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:07:25,516][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:07:25,516][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:07:25,516][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:07:55,331][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:07:55,332][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:08:25,417][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:08:25,417][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:08:55,524][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:08:55,524][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:08:55,524][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:09:25,333][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:09:25,333][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:09:55,655][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:09:55,656][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:09:55,656][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:10:25,338][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:10:25,338][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:10:55,939][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:10:55,939][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:10:55,939][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:11:25,436][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:11:25,436][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:11:55,347][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:11:55,347][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:12:25,347][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:12:25,347][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.8gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:12:25,347][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:12:55,443][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:12:55,443][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:13:25,550][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:13:25,550][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:13:25,550][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:13:55,873][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:13:55,873][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:14:25,348][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:14:25,348][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:14:55,349][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:14:55,349][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:14:55,349][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:15:25,673][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:15:25,674][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:15:55,463][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:15:55,464][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:15:55,464][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:16:25,374][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:16:25,374][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:16:55,964][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:16:55,964][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:16:55,964][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:17:25,473][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:17:25,473][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:17:55,364][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:17:55,364][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:18:25,390][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:18:25,390][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:18:25,390][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:18:55,481][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:18:55,482][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:19:25,372][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:19:25,372][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:19:55,386][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:19:55,386][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:19:55,387][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:20:25,490][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:20:25,490][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:20:55,384][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:20:55,384][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:21:25,395][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:21:25,396][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:21:25,396][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:21:55,387][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:21:55,387][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:22:25,388][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:22:25,389][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:22:55,404][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:22:55,405][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:22:55,405][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:23:25,510][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:23:25,510][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:23:55,511][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:23:55,511][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:23:55,511][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:24:25,508][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:24:25,509][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:24:55,519][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:24:55,519][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:24:55,519][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:25:25,801][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:25:25,801][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:25:55,699][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:25:55,699][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:25:55,699][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:26:25,490][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:26:25,490][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:26:55,634][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:26:55,634][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:27:25,433][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:27:25,433][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:27:25,433][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:27:55,402][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:27:55,402][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:28:25,399][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:28:25,399][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:28:55,405][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:28:55,405][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:28:55,405][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:29:25,409][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:29:25,409][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:29:55,655][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:29:55,655][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:29:55,655][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:30:25,605][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:30:25,605][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:30:55,558][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:30:55,558][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:31:25,414][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:31:25,414][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:31:25,414][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:31:55,462][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:31:55,462][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:32:25,418][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:32:25,418][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:32:25,418][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:32:55,422][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:32:55,422][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:33:25,777][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:33:25,777][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:33:25,777][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:33:55,576][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:33:55,576][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:34:25,680][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:34:25,680][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:34:55,480][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:34:55,480][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:34:55,480][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:35:25,434][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:35:25,434][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:35:56,018][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:35:56,018][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:35:56,018][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:36:25,495][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:36:25,495][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:36:55,436][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:36:55,436][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:37:25,486][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:37:25,486][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:37:25,486][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:37:55,501][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:37:55,501][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:38:25,445][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:38:25,446][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:38:55,801][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:38:55,801][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:38:55,802][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:39:25,513][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:39:25,513][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:39:55,622][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:39:55,622][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:40:25,723][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.8gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:40:25,723][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:40:25,723][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:40:55,464][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.7gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:40:55,464][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:41:25,468][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.7gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:41:25,468][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:41:55,732][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.7gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:41:55,732][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:41:55,733][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:42:25,528][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.7gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:42:25,528][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:42:55,466][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.7gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:42:55,466][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 26.9gb[7.6%], shards will be relocated away from this node
[2015-10-15 20:43:25,738][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.7gb[1.1%], shards will be relocated away from this node
[2015-10-15 20:43:25,738][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.2gb[7.7%], shards will be relocated away from this node
[2015-10-15 20:43:25,739][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:43:55,470][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.7gb[1.1%], shards will be relocated away from this node
[2015-10-15 20:43:55,471][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.2gb[7.7%], shards will be relocated away from this node
[2015-10-15 20:44:25,646][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.7gb[1.1%], shards will be relocated away from this node
[2015-10-15 20:44:25,647][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.2gb[7.7%], shards will be relocated away from this node
[2015-10-15 20:44:56,063][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.7gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:44:56,063][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.2gb[7.7%], shards will be relocated away from this node
[2015-10-15 20:44:56,063][INFO ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 20:45:25,474][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [sT6msm-4TlC6PaXznxGPdQ][Mister Sensitive] free: 2.7gb[1.2%], shards will be relocated away from this node
[2015-10-15 20:45:25,474][WARN ][cluster.routing.allocation.decider] [Kiwi Black] high disk watermark [90%] exceeded on [2jFkYAIWRHWLukCfmqA2Ug][Kiwi Black] free: 27.2gb[7.7%], shards will be relocated away from this node
[2015-10-15 20:45:49,303][INFO ][node                     ] [Kiwi Black] stopping ...
[2015-10-15 20:45:49,345][INFO ][node                     ] [Kiwi Black] stopped
[2015-10-15 20:45:49,346][INFO ][node                     ] [Kiwi Black] closing ...
[2015-10-15 20:45:49,350][INFO ][node                     ] [Kiwi Black] closed
[2015-10-15 21:09:08,924][INFO ][node                     ] [Diamanda Nero] version[1.7.1], pid[6344], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-15 21:09:08,925][INFO ][node                     ] [Diamanda Nero] initializing ...
[2015-10-15 21:09:09,146][INFO ][plugins                  ] [Diamanda Nero] loaded [], sites []
[2015-10-15 21:09:09,247][INFO ][env                      ] [Diamanda Nero] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [151.1gb], net total_space [232.6gb], types [hfs]
[2015-10-15 21:09:13,393][INFO ][node                     ] [Diamanda Nero] initialized
[2015-10-15 21:09:13,393][INFO ][node                     ] [Diamanda Nero] starting ...
[2015-10-15 21:09:13,526][INFO ][transport                ] [Diamanda Nero] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.164:9300]}
[2015-10-15 21:09:13,569][INFO ][discovery                ] [Diamanda Nero] elasticsearch/qexgsAbFQHipdW_9pWw3_A
[2015-10-15 21:09:16,673][INFO ][cluster.service          ] [Diamanda Nero] detected_master [Ravage 2099][lJF1zQsZQkWB3B4Au6UHCg][Beepo-Retina.local][inet[/192.168.1.8:9300]], added {[Ravage 2099][lJF1zQsZQkWB3B4Au6UHCg][Beepo-Retina.local][inet[/192.168.1.8:9300]],}, reason: zen-disco-receive(from master [[Ravage 2099][lJF1zQsZQkWB3B4Au6UHCg][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-15 21:09:16,707][INFO ][http                     ] [Diamanda Nero] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.164:9200]}
[2015-10-15 21:09:16,708][INFO ][node                     ] [Diamanda Nero] started
[2015-10-15 21:09:17,464][WARN ][discovery.zen.ping.multicast] [Diamanda Nero] received ping response ping_response{node [[Mister Sensitive][sT6msm-4TlC6PaXznxGPdQ][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]], id[13], master [[Mister Sensitive][sT6msm-4TlC6PaXznxGPdQ][Nruthyas-MacBook-Air.local][inet[/192.168.1.126:9300]]], hasJoinedOnce [true], cluster_name[elasticsearch]} with no matching id [1]
[2015-10-15 21:09:34,403][INFO ][node                     ] [Diamanda Nero] stopping ...
[2015-10-15 21:09:34,478][INFO ][node                     ] [Diamanda Nero] stopped
[2015-10-15 21:09:34,479][INFO ][node                     ] [Diamanda Nero] closing ...
[2015-10-15 21:09:34,484][INFO ][node                     ] [Diamanda Nero] closed
[2015-10-15 21:09:38,101][INFO ][node                     ] [Ellie Phimster] version[1.7.1], pid[6387], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-15 21:09:38,102][INFO ][node                     ] [Ellie Phimster] initializing ...
[2015-10-15 21:09:38,174][INFO ][plugins                  ] [Ellie Phimster] loaded [], sites []
[2015-10-15 21:09:38,214][INFO ][env                      ] [Ellie Phimster] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [151.1gb], net total_space [232.6gb], types [hfs]
[2015-10-15 21:09:40,573][INFO ][node                     ] [Ellie Phimster] initialized
[2015-10-15 21:09:40,574][INFO ][node                     ] [Ellie Phimster] starting ...
[2015-10-15 21:09:40,654][INFO ][transport                ] [Ellie Phimster] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.164:9300]}
[2015-10-15 21:09:40,677][INFO ][discovery                ] [Ellie Phimster] elasticsearch/tr60yW-RQ92kh_J9JlsjTA
[2015-10-15 21:09:43,971][WARN ][discovery.zen.ping.multicast] [Ellie Phimster] received ping response ping_response{node [[Ravage 2099][lJF1zQsZQkWB3B4Au6UHCg][Beepo-Retina.local][inet[/192.168.1.8:9300]]], id[12], master [[Ravage 2099][lJF1zQsZQkWB3B4Au6UHCg][Beepo-Retina.local][inet[/192.168.1.8:9300]]], hasJoinedOnce [true], cluster_name[elasticsearch]} with no matching id [1]
[2015-10-15 21:09:44,014][INFO ][cluster.service          ] [Ellie Phimster] detected_master [Ravage 2099][lJF1zQsZQkWB3B4Au6UHCg][Beepo-Retina.local][inet[/192.168.1.8:9300]], added {[Ravage 2099][lJF1zQsZQkWB3B4Au6UHCg][Beepo-Retina.local][inet[/192.168.1.8:9300]],}, reason: zen-disco-receive(from master [[Ravage 2099][lJF1zQsZQkWB3B4Au6UHCg][Beepo-Retina.local][inet[/192.168.1.8:9300]]])
[2015-10-15 21:09:44,047][INFO ][http                     ] [Ellie Phimster] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.164:9200]}
[2015-10-15 21:09:44,048][INFO ][node                     ] [Ellie Phimster] started
