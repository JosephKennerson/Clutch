[2015-10-15 00:00:11,460][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:00:11,461][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:00:41,459][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:01:11,464][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:01:11,464][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:01:41,466][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:02:11,469][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:02:11,469][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:02:41,472][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:03:11,475][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:03:11,475][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:03:41,479][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:04:11,479][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:04:11,479][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:04:41,484][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:05:11,487][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:05:11,488][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:05:41,489][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:06:11,490][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:06:11,490][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:06:41,490][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:07:11,489][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:07:11,489][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:07:41,490][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:08:11,491][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:08:11,491][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:08:41,496][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:09:11,497][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:09:11,497][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:09:41,500][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:10:11,501][WARN ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark [90%] exceeded on [P-HVaJV0QC-QTLVeApLBWw][Diamond Lil] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:10:11,501][INFO ][cluster.routing.allocation.decider] [Diamond Lil] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:10:37,895][INFO ][node                     ] [Diamond Lil] stopping ...
[2015-10-15 00:10:37,912][INFO ][node                     ] [Diamond Lil] stopped
[2015-10-15 00:10:37,912][INFO ][node                     ] [Diamond Lil] closing ...
[2015-10-15 00:10:37,916][INFO ][node                     ] [Diamond Lil] closed
[2015-10-15 00:10:52,225][INFO ][node                     ] [Scimitar] version[1.7.1], pid[64222], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-15 00:10:52,226][INFO ][node                     ] [Scimitar] initializing ...
[2015-10-15 00:10:52,293][INFO ][plugins                  ] [Scimitar] loaded [], sites []
[2015-10-15 00:10:52,327][INFO ][env                      ] [Scimitar] using [1] data paths, mounts [[/ (/dev/disk0s2)]], net usable_space [25.3gb], net total_space [352.9gb], types [hfs]
[2015-10-15 00:10:54,277][INFO ][node                     ] [Scimitar] initialized
[2015-10-15 00:10:54,277][INFO ][node                     ] [Scimitar] starting ...
[2015-10-15 00:10:54,340][INFO ][transport                ] [Scimitar] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.8:9300]}
[2015-10-15 00:10:54,358][INFO ][discovery                ] [Scimitar] elasticsearch/e3fw3_EjRvum9iWV9fTcAw
[2015-10-15 00:10:58,136][INFO ][cluster.service          ] [Scimitar] new_master [Scimitar][e3fw3_EjRvum9iWV9fTcAw][Beepo-Retina.local][inet[/192.168.1.8:9300]], reason: zen-disco-join (elected_as_master)
[2015-10-15 00:10:58,167][INFO ][http                     ] [Scimitar] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.8:9200]}
[2015-10-15 00:10:58,168][INFO ][node                     ] [Scimitar] started
[2015-10-15 00:10:58,193][INFO ][gateway                  ] [Scimitar] recovered [2] indices into cluster_state
[2015-10-15 00:11:28,147][WARN ][cluster.routing.allocation.decider] [Scimitar] high disk watermark [90%] exceeded on [e3fw3_EjRvum9iWV9fTcAw][Scimitar] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:11:28,147][INFO ][cluster.routing.allocation.decider] [Scimitar] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:11:58,144][WARN ][cluster.routing.allocation.decider] [Scimitar] high disk watermark [90%] exceeded on [e3fw3_EjRvum9iWV9fTcAw][Scimitar] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:12:28,147][WARN ][cluster.routing.allocation.decider] [Scimitar] high disk watermark [90%] exceeded on [e3fw3_EjRvum9iWV9fTcAw][Scimitar] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:12:28,147][INFO ][cluster.routing.allocation.decider] [Scimitar] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:12:50,771][DEBUG][action.index             ] [Scimitar] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-10-15 00:12:58,151][WARN ][cluster.routing.allocation.decider] [Scimitar] high disk watermark [90%] exceeded on [e3fw3_EjRvum9iWV9fTcAw][Scimitar] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:13:28,155][WARN ][cluster.routing.allocation.decider] [Scimitar] high disk watermark [90%] exceeded on [e3fw3_EjRvum9iWV9fTcAw][Scimitar] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:13:28,156][INFO ][cluster.routing.allocation.decider] [Scimitar] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:13:36,272][INFO ][node                     ] [Scimitar] stopping ...
[2015-10-15 00:13:36,284][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
	at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)
	at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
	at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
	at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
	at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)
	at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
	at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.finishAsFailed(TransportShardReplicationOperationAction.java:536)
	at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$3.onClusterServiceClose(TransportShardReplicationOperationAction.java:509)
	at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onClose(ClusterStateObserver.java:217)
	at org.elasticsearch.cluster.service.InternalClusterService.doStop(InternalClusterService.java:174)
	at org.elasticsearch.common.component.AbstractLifecycleComponent.stop(AbstractLifecycleComponent.java:105)
	at org.elasticsearch.node.internal.InternalNode.stop(InternalNode.java:310)
	at org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:334)
	at org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:82)
[2015-10-15 00:13:36,294][INFO ][node                     ] [Scimitar] stopped
[2015-10-15 00:13:36,294][INFO ][node                     ] [Scimitar] closing ...
[2015-10-15 00:13:36,298][INFO ][node                     ] [Scimitar] closed
[2015-10-15 00:14:30,277][INFO ][node                     ] [Justin Hammer] version[1.7.1], pid[64862], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-15 00:14:30,278][INFO ][node                     ] [Justin Hammer] initializing ...
[2015-10-15 00:14:30,343][INFO ][plugins                  ] [Justin Hammer] loaded [], sites []
[2015-10-15 00:14:30,377][INFO ][env                      ] [Justin Hammer] using [1] data paths, mounts [[/ (/dev/disk0s2)]], net usable_space [25.3gb], net total_space [352.9gb], types [hfs]
[2015-10-15 00:14:32,326][INFO ][node                     ] [Justin Hammer] initialized
[2015-10-15 00:14:32,327][INFO ][node                     ] [Justin Hammer] starting ...
[2015-10-15 00:14:32,388][INFO ][transport                ] [Justin Hammer] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.8:9300]}
[2015-10-15 00:14:32,404][INFO ][discovery                ] [Justin Hammer] elasticsearch/3StrIZ3CQbm8qH4vW5aBLQ
[2015-10-15 00:14:36,183][INFO ][cluster.service          ] [Justin Hammer] new_master [Justin Hammer][3StrIZ3CQbm8qH4vW5aBLQ][Beepo-Retina.local][inet[/192.168.1.8:9300]], reason: zen-disco-join (elected_as_master)
[2015-10-15 00:14:36,216][INFO ][http                     ] [Justin Hammer] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.8:9200]}
[2015-10-15 00:14:36,217][INFO ][node                     ] [Justin Hammer] started
[2015-10-15 00:14:36,238][INFO ][gateway                  ] [Justin Hammer] recovered [2] indices into cluster_state
[2015-10-15 00:15:06,196][WARN ][cluster.routing.allocation.decider] [Justin Hammer] high disk watermark [90%] exceeded on [3StrIZ3CQbm8qH4vW5aBLQ][Justin Hammer] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:15:06,196][INFO ][cluster.routing.allocation.decider] [Justin Hammer] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:15:36,191][WARN ][cluster.routing.allocation.decider] [Justin Hammer] high disk watermark [90%] exceeded on [3StrIZ3CQbm8qH4vW5aBLQ][Justin Hammer] free: 25.2gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:15:38,854][DEBUG][action.index             ] [Justin Hammer] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-10-15 00:16:06,196][WARN ][cluster.routing.allocation.decider] [Justin Hammer] high disk watermark [90%] exceeded on [3StrIZ3CQbm8qH4vW5aBLQ][Justin Hammer] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:16:06,196][INFO ][cluster.routing.allocation.decider] [Justin Hammer] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:16:26,266][INFO ][node                     ] [Justin Hammer] stopping ...
[2015-10-15 00:16:26,278][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
	at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)
	at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
	at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
	at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
	at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)
	at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
	at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.finishAsFailed(TransportShardReplicationOperationAction.java:536)
	at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$3.onClusterServiceClose(TransportShardReplicationOperationAction.java:509)
	at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onClose(ClusterStateObserver.java:217)
	at org.elasticsearch.cluster.service.InternalClusterService.doStop(InternalClusterService.java:174)
	at org.elasticsearch.common.component.AbstractLifecycleComponent.stop(AbstractLifecycleComponent.java:105)
	at org.elasticsearch.node.internal.InternalNode.stop(InternalNode.java:310)
	at org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:334)
	at org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:82)
[2015-10-15 00:16:26,287][INFO ][node                     ] [Justin Hammer] stopped
[2015-10-15 00:16:26,288][INFO ][node                     ] [Justin Hammer] closing ...
[2015-10-15 00:16:26,292][INFO ][node                     ] [Justin Hammer] closed
[2015-10-15 00:52:12,116][INFO ][node                     ] [Bedlam] version[1.7.1], pid[1538], build[b88f43f/2015-07-29T09:54:16Z]
[2015-10-15 00:52:12,116][INFO ][node                     ] [Bedlam] initializing ...
[2015-10-15 00:52:12,235][INFO ][plugins                  ] [Bedlam] loaded [], sites []
[2015-10-15 00:52:12,293][INFO ][env                      ] [Bedlam] using [1] data paths, mounts [[/ (/dev/disk0s2)]], net usable_space [25.4gb], net total_space [352.9gb], types [hfs]
[2015-10-15 00:52:16,792][INFO ][node                     ] [Bedlam] initialized
[2015-10-15 00:52:16,793][INFO ][node                     ] [Bedlam] starting ...
[2015-10-15 00:52:17,239][INFO ][transport                ] [Bedlam] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.8:9300]}
[2015-10-15 00:52:17,277][INFO ][discovery                ] [Bedlam] elasticsearch/V_X2kzZmRbejXbcvdoPnTg
[2015-10-15 00:52:21,062][INFO ][cluster.service          ] [Bedlam] new_master [Bedlam][V_X2kzZmRbejXbcvdoPnTg][Beepo-Retina.local][inet[/192.168.1.8:9300]], reason: zen-disco-join (elected_as_master)
[2015-10-15 00:52:21,084][INFO ][http                     ] [Bedlam] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.8:9200]}
[2015-10-15 00:52:21,085][INFO ][node                     ] [Bedlam] started
[2015-10-15 00:52:21,122][INFO ][gateway                  ] [Bedlam] recovered [2] indices into cluster_state
[2015-10-15 00:52:51,074][WARN ][cluster.routing.allocation.decider] [Bedlam] high disk watermark [90%] exceeded on [V_X2kzZmRbejXbcvdoPnTg][Bedlam] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:52:51,074][INFO ][cluster.routing.allocation.decider] [Bedlam] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:53:21,072][WARN ][cluster.routing.allocation.decider] [Bedlam] high disk watermark [90%] exceeded on [V_X2kzZmRbejXbcvdoPnTg][Bedlam] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:53:51,074][WARN ][cluster.routing.allocation.decider] [Bedlam] high disk watermark [90%] exceeded on [V_X2kzZmRbejXbcvdoPnTg][Bedlam] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:53:51,074][INFO ][cluster.routing.allocation.decider] [Bedlam] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-10-15 00:54:21,079][WARN ][cluster.routing.allocation.decider] [Bedlam] high disk watermark [90%] exceeded on [V_X2kzZmRbejXbcvdoPnTg][Bedlam] free: 25.3gb[7.1%], shards will be relocated away from this node
[2015-10-15 00:54:40,336][INFO ][node                     ] [Bedlam] stopping ...
[2015-10-15 00:54:40,360][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
	at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)
	at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
	at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
	at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
	at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)
	at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
	at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.finishAsFailed(TransportShardReplicationOperationAction.java:536)
	at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$3.onClusterServiceClose(TransportShardReplicationOperationAction.java:509)
	at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onClose(ClusterStateObserver.java:217)
	at org.elasticsearch.cluster.service.InternalClusterService.doStop(InternalClusterService.java:174)
	at org.elasticsearch.common.component.AbstractLifecycleComponent.stop(AbstractLifecycleComponent.java:105)
	at org.elasticsearch.node.internal.InternalNode.stop(InternalNode.java:310)
	at org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:334)
	at org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:82)
[2015-10-15 00:54:40,374][INFO ][node                     ] [Bedlam] stopped
[2015-10-15 00:54:40,374][INFO ][node                     ] [Bedlam] closing ...
[2015-10-15 00:54:40,380][INFO ][node                     ] [Bedlam] closed
